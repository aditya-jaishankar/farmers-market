{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Dirichlet Allocation\n",
    "\n",
    "This is the main chunk of the code.\n",
    "\n",
    "The eventual goal is to treat the hashtag list for each user as being document1, and the cleaned full-text words as being document 2. So each user has two documents. Now I do topic modeling across each document for each user and for each user find a list of topics, and then the words that lie within each topic. Therefore, I have now for each user a dictionary with keys as topics and values as the words associated with each topic. What I am then hoping to do is some sort of visualization to extract the most relevant topics that exhibit the words that I am interested in. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports\n",
    "import json\n",
    "import glob\n",
    "import pickle\n",
    "import collections\n",
    "import random\n",
    "from tqdm import tqdm as tqdm\n",
    "import config\n",
    "import time\n",
    "import os\n",
    "dirpath = os.path.dirname(os.path.realpath('__file__'))\n",
    "from pprint import pprint\n",
    "\n",
    "# import logging\n",
    "# logging.basicConfig(format='%(levelname)s : %(message)s', level=logging.INFO)\n",
    "# logging.root.level = logging.INFO\n",
    "\n",
    "# NLP imports\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['https', 'http', 'shit', 'shitting',\n",
    "                    'london', 'para', 'fuck', 'fucking', 'bitch'])\n",
    "import re\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.models import CoherenceModel\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "\n",
    "# To check later if a words is in english or not\n",
    "with open('./words_dictionary.json') as filehandle:\n",
    "    words_dictionary = json.load(filehandle)\n",
    "english_words = words_dictionary.keys()\n",
    "\n",
    "# Visualization imports\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "pyLDAvis.enable_notebook()\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Other imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tweepy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the cleaned and simplified tweet dictionary\n",
    "\n",
    "### Note on the format of input and output dictionaries\n",
    "\n",
    "Here, we first load in the dictionaries that were dumped in as pickle files and then do a series of text processing and cleaning tasks. I initially start with a dicitonary of the form:\n",
    "\n",
    "```\n",
    "{\n",
    "    market_1: {\n",
    "                screen_name_1: [{tweet1, ..., tweetn}],\n",
    "                .\n",
    "                .\n",
    "                .\n",
    "                screen_name_m: [{tweet1, ..., tweetn}]\n",
    "             }\n",
    "    .\n",
    "    .\n",
    "    .\n",
    "    market_k: {\n",
    "                screen_name_1: [{tweet1, ..., tweetn}],\n",
    "                .\n",
    "                .\n",
    "                .\n",
    "                screen_name_m: [{tweet1, ..., tweetn}]\n",
    "             }\n",
    "}\n",
    "```\n",
    "\n",
    "This section of the code will then process will result in a dictionary of the form\n",
    "\n",
    "```\n",
    "{\n",
    "    market_1: {\n",
    "                screen_name_1: \n",
    "                    {\n",
    "                        hashtags: [list of hashtags from each tweet], \n",
    "                        fulltext: [list of all cleaned/depunkt words across all tweets]\n",
    "                    },\n",
    "                .\n",
    "                .\n",
    "                screen_name_m: \n",
    "                    {\n",
    "                        hashtags: [list of hashtags from each tweet], \n",
    "                        fulltext: [list of all cleaned/depunkt words across all tweets]\n",
    "                    }\n",
    "              }\n",
    "    .\n",
    "    .\n",
    "    .\n",
    "    market_k: {\n",
    "                screen_name_1: \n",
    "                    {\n",
    "                        hashtags: [list of hashtags from each tweet], \n",
    "                        fulltext: [list of all cleaned/depunkt words across all tweets]\n",
    "                    },\n",
    "                .\n",
    "                .\n",
    "                screen_name_m: \n",
    "                    {\n",
    "                        hashtags: [list of hashtags from each tweet], \n",
    "                        fulltext: [list of all cleaned/depunkt words across all tweets]\n",
    "                    }\n",
    "              }\n",
    "}\n",
    "```\n",
    "\n",
    "Then I can turn this into a pandas dataframe and do some pretty nice data manipulation.\n",
    "\n",
    "We will call this dictionary the `master_dict`.\n",
    "\n",
    "To do this, we first define some helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Defining some utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user(tweet):\n",
    "    \"\"\"\n",
    "    input: tweet dictionary\n",
    "    returns: return the username\n",
    "    \"\"\"\n",
    "    return tweet['user']['screen_name']\n",
    "\n",
    "\n",
    "def get_hashtag_list(tweet):\n",
    "    \"\"\"\n",
    "    input: tweet dictionary\n",
    "    returns: list of all hashtags in both the direct tweet and the\n",
    "    retweet \n",
    "    \"\"\"\n",
    "\n",
    "    l = []\n",
    "    for d in tweet['entities']['hashtags']:\n",
    "        l += [d['text']]\n",
    "\n",
    "    if 'retweeted_status' in tweet.keys():\n",
    "        for d in tweet['retweeted_status']['entities']['hashtags']:\n",
    "            l += [d['text']]\n",
    "    return l\n",
    "\n",
    "\n",
    "def tokenizer_cleaner_nostop_lemmatizer(text):\n",
    "    \"\"\"\n",
    "    This function tokenizes the text of a tweet, cleans it off punctuation,\n",
    "    removes stop words, and lemmatizes the words (i.e. finds word roots to remove noise)\n",
    "    I am largely using the gensim and spacy packages \n",
    "\n",
    "    Input: Some text\n",
    "    Output: List of tokenized, cleaned, lemmatized words\n",
    "    \"\"\"\n",
    "\n",
    "    tokenized_depunkt = gensim.utils.simple_preprocess(text, min_len=4, deacc=True)\n",
    "    tokenized_depunkt_nostop = ([word for word in tokenized_depunkt \n",
    "                                 if (word not in stop_words and word in english_words)])\n",
    "    \n",
    "    # Lemmatizer while also only allowing certain parts of speech.\n",
    "    # See here: https://spacy.io/api/annotation\n",
    "    allowed_pos = ['ADJ', 'ADV', 'NOUN', 'PROPN','VERB']\n",
    "    doc = nlp(' '.join(tokenized_depunkt_nostop))\n",
    "    words_final = [token.lemma_ for token in doc if token.pos_ in allowed_pos]\n",
    "    return words_final\n",
    "\n",
    "    \n",
    "def get_tweet_words_list(tweet):\n",
    "    \"\"\"\n",
    "    This function takes in a tweet and checks if there is a retweet associated with it\n",
    "    input: tweet\n",
    "    output: list of tokenized words without punctuation\n",
    "    \"\"\"\n",
    "\n",
    "    text = tweet['full_text']\n",
    "    clean_words = tokenizer_cleaner_nostop_lemmatizer(text)\n",
    "    \n",
    "    if 'retweeted_status' in tweet.keys():\n",
    "        retweet_text = tweet['retweeted_status']['full_text']\n",
    "        retweet_clean_words = tokenizer_cleaner_nostop_lemmatizer(retweet_text)\n",
    "        clean_words += retweet_clean_words\n",
    "    return clean_words"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating the `master_dict`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('./data/all_tweets_dict.data', 'rb') as filehandle:\n",
    "#     all_tweets_data = pickle.load(filehandle)\n",
    "\n",
    "# master_dict = {}\n",
    "\n",
    "# for market in all_tweets_data:\n",
    "#     followers = all_tweets_data[market]\n",
    "#     master_dict[market] = {}\n",
    "\n",
    "#     for follower in tqdm(followers):\n",
    "#         tweets = all_tweets_data[market][follower] # list of tweet_.json\n",
    "#         master_dict[market][follower] = {}\n",
    "#         master_dict[market][follower]['hashtags'] = []\n",
    "#         master_dict[market][follower]['fulltext'] = []\n",
    "#         for tweet in tweets:\n",
    "#             hashtags = get_hashtag_list(tweet)\n",
    "#             words = get_tweet_words_list(tweet)\n",
    "            \n",
    "#             master_dict[market][follower]['hashtags'].extend(hashtags)\n",
    "#             master_dict[market][follower]['fulltext'].extend(words)\n",
    "\n",
    "# with open('./data/master_dict.data', 'wb') as filehandle:\n",
    "#     pickle.dump(master_dict, filehandle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the Latent Dirichlet Allocation\n",
    "\n",
    "Now we apply the LDA algorithm to identify themes in the documents/topics. In my case, a single document corresponds to the set of all words of a single user's tweets. Note that the list of words that comprose a document have already been cleaned, tokenized and lemmatized. \n",
    "\n",
    "One other thought is to have one MASSIVE document containing all tweets of all users, and then finding the topics there. In the comparison step, I could use these top topics and then compare this to all tweets of individual users and then returning top-k users based on similarity. See [this](https://stats.stackexchange.com/questions/269031/how-to-find-similar-documents-after-a-latent-dirichlet-allocation-model-is-bui) stack-exchange post for ideas.\n",
    "\n",
    "We first write some helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/master_dict.data', 'rb') as filehandle:\n",
    "    master_dict = pickle.load(filehandle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_docs(d, market):\n",
    "    \"\"\"\n",
    "    Accepts a market and then returns the documents for the market. A document\n",
    "    is a list of of word lists for each user in the market city i.e. it is a list of lists.\n",
    "    Each outer list is a follower and the innner list is the cleaner, tokenized, depunkt, \n",
    "    lematized set of words for that follower.\n",
    "    \"\"\"\n",
    "    docs = []\n",
    "    for user in d[market]:\n",
    "        text_list = d[market][user]['fulltext']\n",
    "        docs.append(text_list)\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "markets = list(master_dict.keys())\n",
    "market_index = 2\n",
    "docs = get_docs(master_dict, markets[market_index])\n",
    "id2word = corpora.Dictionary(docs)\n",
    "\n",
    "# Idea: Keep only those tokens that appear in at least 10% of the documents\n",
    "id2word.filter_extremes(no_below=int(0.1*len(docs)))\n",
    "corpus = [id2word.doc2bow(doc) for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "time:119.46721744537354\nC:\\Users\\adity\\AppData\\Local\\Continuum\\anaconda3\\envs\\insightenv\\lib\\site-packages\\smart_open\\smart_open_lib.py:402: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\nC:\\Users\\adity\\AppData\\Local\\Continuum\\anaconda3\\envs\\insightenv\\lib\\site-packages\\smart_open\\smart_open_lib.py:402: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n[(0,\n'0.108*\"climate\" + 0.050*\"sustainable\" + 0.044*\"plastic\" + 0.042*\"para\" + '\n'0.039*\"planet\" + 0.022*\"fuel\" + 0.022*\"waste\" + 0.021*\"global\" + '\n'0.019*\"ocean\" + 0.018*\"fossil\"'),\n(1,\n'0.007*\"literally\" + 0.007*\"pain\" + 0.006*\"shit\" + 0.006*\"fuck\" + '\n'0.005*\"brain\" + 0.005*\"burn\" + 0.005*\"doctor\" + 0.005*\"sleep\" + '\n'0.005*\"character\" + 0.005*\"wake\"'),\n(2,\n'0.009*\"senate\" + 0.009*\"donald\" + 0.009*\"election\" + 0.009*\"elizabeth\" + '\n'0.008*\"americans\" + 0.008*\"attack\" + 0.008*\"impeachment\" + 0.008*\"congress\" '\n'+ 0.008*\"senator\" + 0.008*\"justice\"'),\n(3,\n'0.012*\"tech\" + 0.008*\"technology\" + 0.008*\"patient\" + 0.007*\"education\" + '\n'0.006*\"hospital\" + 0.005*\"insurance\" + 0.005*\"marketing\" + 0.005*\"vision\" + '\n'0.005*\"apply\" + 0.005*\"college\"'),\n(4,\n'0.091*\"farm\" + 0.086*\"farmer\" + 0.085*\"garden\" + 0.049*\"organic\" + '\n'0.039*\"soil\" + 0.029*\"plant\" + 0.027*\"agriculture\" + 0.024*\"repost\" + '\n'0.022*\"farming\" + 0.020*\"compost\"'),\n(5,\n'0.303*\"network\" + 0.260*\"entrepreneur\" + 0.116*\"customer\" + 0.061*\"virtual\" '\n'+ 0.047*\"expand\" + 0.039*\"success\" + 0.034*\"successful\" + '\n'0.015*\"innovative\" + 0.014*\"strategy\" + 0.005*\"incorporate\"'),\n(6,\n'0.071*\"chef\" + 0.031*\"menu\" + 0.029*\"lunch\" + 0.027*\"sushi\" + '\n'0.026*\"kitchen\" + 0.025*\"west\" + 0.024*\"japanese\" + 0.013*\"hotel\" + '\n'0.013*\"meal\" + 0.013*\"software\"'),\n(7,\n'0.012*\"brooklyn\" + 0.009*\"donate\" + 0.009*\"cancer\" + 0.007*\"manhattan\" + '\n'0.007*\"volunteer\" + 0.006*\"resident\" + 0.006*\"refugee\" + 0.006*\"science\" + '\n'0.006*\"diet\" + 0.005*\"bike\"'),\n(8,\n'0.013*\"song\" + 0.011*\"artist\" + 0.008*\"nature\" + 0.008*\"beauty\" + '\n'0.007*\"image\" + 0.006*\"museum\" + 0.006*\"beach\" + 0.006*\"alive\" + '\n'0.006*\"photography\" + 0.006*\"sunset\"'),\n(9,\n'0.037*\"wine\" + 0.029*\"recipe\" + 0.025*\"cook\" + 0.025*\"delicious\" + '\n'0.020*\"taste\" + 0.018*\"noodle\" + 0.018*\"vegan\" + 0.018*\"dish\" + '\n'0.016*\"cooking\" + 0.013*\"flavor\"')]\n"
    }
   ],
   "source": [
    "def compute_lda(corpus, id2word, k=10, alpha='auto'):\n",
    "    \"\"\"\n",
    "    Performs the LDA and returns the computer model.\n",
    "    Input: Corpus, dictionary and hyperparameters to optimize\n",
    "    Output: the fitted/computed LDA model\n",
    "    \"\"\"\n",
    "    lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus, \n",
    "                                                id2word=id2word,\n",
    "                                                num_topics=k,\n",
    "                                                random_state=100,\n",
    "                                                # update_every=1,\n",
    "                                                chunksize=5,\n",
    "                                                passes=100,\n",
    "                                                alpha=.01,\n",
    "                                                iterations=100,\n",
    "                                                per_word_topics=True)\n",
    "    return lda_model\n",
    "t1 = time.time()\n",
    "lda_model = compute_lda(corpus, id2word)\n",
    "t2 = time.time()\n",
    "print('time:', t2-t1)\n",
    "# save the model\n",
    "filename_model = './ldamodels/market' + str(market_index) + '/model.model'\n",
    "lda_model.save(filename_model)\n",
    "# save the corpus\n",
    "filename_corpus = './ldamodels/market' + str(market_index) + '/corpus.corpus'\n",
    "with open(filename_corpus, 'wb') as filehandle:\n",
    "    pickle.dump(corpus, filehandle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "pprint(lda_model.print_topics())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some comments on the hyperparameter tuning:\n",
    "\n",
    "1. Doing a chunksize of 1 is pretty slow and time consuming (although it might be worthwhile to time this more accurately). I think either choosing `chunksize=5` or `chunksize=10` works well.\n",
    "2. `passes` is a parameter similar to number of epochs. \n",
    "3. `alpha='auto'` seems to work pretty well. \n",
    "4. Keep `random_state=100` in case you want to repeat results. \n",
    "5. Keep `update_every` small, ideally equal to 1. \n",
    "6. Use a for loop to calculate the optimal number of topics. This just has to be done. Might be better to do this in the background on the python terminal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating the Coherence score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Coherence Score:0.5412639729797816\n"
    }
   ],
   "source": [
    "coherence_model_lda = CoherenceModel(model=lda_model,\n",
    "                                     texts=docs,\n",
    "                                     dictionary=id2word,\n",
    "                                     coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "\n",
    "print('Coherence Score:', coherence_lda)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "C:\\Users\\adity\\AppData\\Local\\Continuum\\anaconda3\\envs\\insightenv\\lib\\site-packages\\pyLDAvis\\_prepare.py:257: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\nof pandas will change to not sort by default.\n\nTo accept the future behavior, pass 'sort=False'.\n\nTo retain the current behavior and silence the warning, pass 'sort=True'.\n\n  return pd.concat([default_term_info] + list(topic_dfs))\nLDAvis prep time:63.65331959724426\n\nNote: if you're in the IPython notebook, pyLDAvis.show() is not the best command\n      to use. Consider using pyLDAvis.display(), or pyLDAvis.enable_notebook().\n      See more information at http://pyLDAvis.github.io/quickstart.html .\n\nYou must interrupt the kernel to end this command\n\nServing to http://127.0.0.1:8889/    [Ctrl-C to exit]\n127.0.0.1 - - [23/Jan/2020 14:07:55] \"GET / HTTP/1.1\" 200 -\n127.0.0.1 - - [23/Jan/2020 14:07:55] \"GET /LDAvis.css HTTP/1.1\" 200 -\n127.0.0.1 - - [23/Jan/2020 14:07:55] \"GET /d3.js HTTP/1.1\" 200 -\n127.0.0.1 - - [23/Jan/2020 14:07:55] \"GET /LDAvis.js HTTP/1.1\" 200 -\n\nstopping Server...\n"
    }
   ],
   "source": [
    "t1 = time.time()\n",
    "LDAvis_prepared = pyLDAvis.gensim.prepare(lda_model, corpus, dictionary=lda_model.id2word, mds='tsne')\n",
    "t2 = time.time()\n",
    "print('LDAvis prep time:', t2-t1)\n",
    "pyLDAvis.show(LDAvis_prepared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3.6.8 64-bit ('insightenv': conda)",
   "language": "python",
   "name": "python36864bitinsightenvconda4d948c2035f94dc3b09753f05f123a75"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8-final"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}