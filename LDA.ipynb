{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.6.8-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python36864bitinsightenvconda4d948c2035f94dc3b09753f05f123a75",
   "display_name": "Python 3.6.8 64-bit ('insightenv': conda)"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Latent Dirichlet Allocation\n",
    "\n",
    "This is the main chunk of the code.\n",
    "\n",
    "The eventual goal is to treat the hashtag list for each user as being document1, and the cleaned full-text words as being document 2. So each user has two documents. Now I do topic modeling across each document for each user and for each user find a list of topics, and then the words that lie within each topic. Therefore, I have now for each user a dictionary with keys as topics and values as the words associated with each topic. What I am then hoping to do is some sort of visualization to extract the most relevant topics that exhibit the words that I am interested in. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports\n",
    "import json\n",
    "import glob\n",
    "import pickle\n",
    "import collections\n",
    "import random\n",
    "from tqdm import tqdm as tqdm\n",
    "import config\n",
    "import os\n",
    "dirpath = os.path.dirname(os.path.realpath('__file__'))\n",
    "\n",
    "# NLP imports\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'http', 'https'])\n",
    "import re\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.models import CoherenceModel\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "# To check later if a words is in english or not\n",
    "with open('./english-words.txt', 'rb') as filehandle:\n",
    "    english_words = filehandle.readlines()\n",
    "\n",
    "# Visualization imports\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Other imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tweepy"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating the cleaned and simplified tweet dictionary\n",
    "\n",
    "Here, we first load in the dictionaries that were dumped in as pickle files and then do a series of text processing and cleaning tasks. I initially start with a dicitonary of the form:\n",
    "\n",
    "```\n",
    "{\n",
    "    user1: [{tweet1_json}, {tweet2_json}, ..., {tweetn_json}]\n",
    "    .\n",
    "    .\n",
    "    .\n",
    "    usern: [{tweet1_json}, {tweet2_json}, ..., {tweetn_json}]\n",
    "}\n",
    "```\n",
    "\n",
    "This section of the code will then process will result in a dictionary of the form\n",
    "\n",
    "```\n",
    "{\n",
    "    user1: \n",
    "        {\n",
    "            hashtags: [list of hashtags from each tweet], \n",
    "            fulltext: [list of all cleaned/depunkt words across all tweets]\n",
    "        },\n",
    "    .\n",
    "    .\n",
    "    .\n",
    "    usern: \n",
    "        {\n",
    "            hashtags: [list of hashtags from each tweet], \n",
    "            fulltext: [list of all cleaned/depunkt words across all tweets]\n",
    "        }\n",
    "}\n",
    "```\n",
    "\n",
    "Then I can turn this into a pandas dataframe and do some pretty nice data manipulation.\n",
    "\n",
    "We will call this dictionary the `master_dict`.\n",
    "\n",
    "To do this, we first define some helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user(tweet):\n",
    "    \"\"\"\n",
    "    input: tweet dictionary\n",
    "    returns: return the username\n",
    "    \"\"\"\n",
    "    return tweet['user']['screen_name']\n",
    "\n",
    "\n",
    "def get_hashtag_list(tweet):\n",
    "    \"\"\"\n",
    "    input: tweet dictionary\n",
    "    returns: list of all hashtags in both the direct tweet and the\n",
    "    retweet \n",
    "    \"\"\"\n",
    "\n",
    "    l = []\n",
    "    for d in tweet['entities']['hashtags']:\n",
    "        l += [d['text']]\n",
    "\n",
    "    if 'retweeted_status' in tweet.keys():\n",
    "        for d in tweet['retweeted_status']['entities']['hashtags']:\n",
    "            l += [d['text']]\n",
    "    return l\n",
    "\n",
    "\n",
    "def tokenizer_cleaner_nostop_lemmatizer(text):\n",
    "    \"\"\"\n",
    "    This function tokenizes the text of a tweet, cleans it off punctuation,\n",
    "    removes stop words, and lemmatizes the words (i.e. finds word roots to remove noise)\n",
    "    I am largely using the gensim and spacy packages \n",
    "\n",
    "    Input: Some text\n",
    "    Output: List of tokenized, cleaned, lemmatized words\n",
    "    \"\"\"\n",
    "\n",
    "    tokenized_depunkt = gensim.utils.simple_preprocess(text, min_len=4, deacc=True)\n",
    "    tokenized_depunkt_nostop = ([word for word in tokenized_depunkt \n",
    "                                 if word not in stop_words and word in english_words])\n",
    "    \n",
    "    # Lemmatizer while also only allowing certain parts of speech.\n",
    "    # See here: https://spacy.io/api/annotation\n",
    "    allowed_pos = ['ADJ', 'ADV', 'NOUN', 'PROPN','VERB']\n",
    "    doc = nlp(' '.join(tokenized_depunkt_nostop))\n",
    "    words_final = [token.lemma_ for token in doc if token.pos_ in allowed_pos]\n",
    "    return words_final\n",
    "\n",
    "    \n",
    "def get_tweet_words_list(tweet):\n",
    "    \"\"\"\n",
    "    This function takes in a tweet and checks if there is a retweet associated with it\n",
    "    input: tweet\n",
    "    output: list of tokenized words without punctuation\n",
    "    \"\"\"\n",
    "\n",
    "    text = tweet['full_text']\n",
    "    clean_words = tokenizer_cleaner_nostop_lemmatizer(text)\n",
    "    \n",
    "    if 'retweeted_status' in tweet.keys():\n",
    "        retweet_text = tweet['retweeted_status']['full_text']\n",
    "        retweet_clean_words = tokenizer_cleaner_nostop_lemmatizer(retweet_text)\n",
    "        clean_words += retweet_clean_words\n",
    "    return clean_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": " 52%|█████▎    | 105/200 [3:09:22<1:23:57, 53.03s/it]"
    }
   ],
   "source": [
    "with open('./all_tweets_dict.data', 'rb') as filehandle:\n",
    "    all_tweets_data = pickle.load(filehandle)\n",
    "\n",
    "master_dict = collections.defaultdict(lambda: {})\n",
    "users = list(all_tweets_data.keys())[:200]\n",
    "for user in tqdm(users):\n",
    "    user_tweets = all_tweets_data[user]\n",
    "    for tweet in user_tweets: # tweet is a json_object\n",
    "        master_dict[user]['hashtags'] = (master_dict[user].get('hashtags', [])\n",
    "                                      + get_hashtag_list(tweet))\n",
    "        master_dict[user]['tweet_words'] = (master_dict[user].get('tweet_words', [])\n",
    "                                         + get_tweet_words_list(tweet))\n",
    "\n",
    "# Go back and do this later for all the 1000 documents\n",
    "with open('./master_dict.data', 'wb') as filehandle:\n",
    "    pickle.dump(dict(master_dict), filehandle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Latent Dirichlet Allocation\n",
    "\n",
    "Now we apply the LDA algorithm to identify themes in the documents/topics. In my case, a single document corresponds to the set of all words of a single user's tweets. Note that the list of words that comprose a document have already been cleaned, tokenized and lemmatized. \n",
    "\n",
    "One other thought is to have one MASSIVE document containing all tweets of all users, and then finding the topics there. In the comparison step, I could use these top topics and then compare this to all tweets of individual users and then returning top-k users based on similarity. See [this](https://stats.stackexchange.com/questions/269031/how-to-find-similar-documents-after-a-latent-dirichlet-allocation-model-is-bui) stack-exchange post for ideas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}