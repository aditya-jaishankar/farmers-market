{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Dirichlet Allocation\n",
    "\n",
    "This is the main chunk of the code.\n",
    "\n",
    "The eventual goal is to treat the hashtag list for each user as being document1, and the cleaned full-text words as being document 2. So each user has two documents. Now I do topic modeling across each document for each user and for each user find a list of topics, and then the words that lie within each topic. Therefore, I have now for each user a dictionary with keys as topics and values as the words associated with each topic. What I am then hoping to do is some sort of visualization to extract the most relevant topics that exhibit the words that I am interested in. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports\n",
    "import json\n",
    "import glob\n",
    "import pickle\n",
    "import collections\n",
    "import random\n",
    "from tqdm import tqdm as tqdm\n",
    "import config\n",
    "import time\n",
    "import os\n",
    "dirpath = os.path.dirname(os.path.realpath('__file__'))\n",
    "from pprint import pprint\n",
    "\n",
    "# import logging\n",
    "# logging.basicConfig(format='%(levelname)s : %(message)s', level=logging.INFO)\n",
    "# logging.root.level = logging.INFO\n",
    "\n",
    "# NLP imports\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['https', 'http'])\n",
    "import re\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.models import CoherenceModel\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "\n",
    "# To check later if a words is in english or not\n",
    "with open('./words_dictionary.json') as filehandle:\n",
    "    words_dictionary = json.load(filehandle)\n",
    "english_words = words_dictionary.keys()\n",
    "\n",
    "# Visualization imports\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "pyLDAvis.enable_notebook()\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Other imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tweepy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the cleaned and simplified tweet dictionary\n",
    "\n",
    "### Note on the format of input and output dictionaries\n",
    "\n",
    "Here, we first load in the dictionaries that were dumped in as pickle files and then do a series of text processing and cleaning tasks. I initially start with a dicitonary of the form:\n",
    "\n",
    "```\n",
    "{\n",
    "    market_1: {\n",
    "                screen_name_1: [{tweet1, ..., tweetn}],\n",
    "                .\n",
    "                .\n",
    "                .\n",
    "                screen_name_m: [{tweet1, ..., tweetn}]\n",
    "             }\n",
    "    .\n",
    "    .\n",
    "    .\n",
    "    market_k: {\n",
    "                screen_name_1: [{tweet1, ..., tweetn}],\n",
    "                .\n",
    "                .\n",
    "                .\n",
    "                screen_name_m: [{tweet1, ..., tweetn}]\n",
    "             }\n",
    "}\n",
    "```\n",
    "\n",
    "This section of the code will then process will result in a dictionary of the form\n",
    "\n",
    "```\n",
    "{\n",
    "    market_1: {\n",
    "                screen_name_1: \n",
    "                    {\n",
    "                        hashtags: [list of hashtags from each tweet], \n",
    "                        fulltext: [list of all cleaned/depunkt words across all tweets]\n",
    "                    },\n",
    "                .\n",
    "                .\n",
    "                screen_name_m: \n",
    "                    {\n",
    "                        hashtags: [list of hashtags from each tweet], \n",
    "                        fulltext: [list of all cleaned/depunkt words across all tweets]\n",
    "                    }\n",
    "              }\n",
    "    .\n",
    "    .\n",
    "    .\n",
    "    market_k: {\n",
    "                screen_name_1: \n",
    "                    {\n",
    "                        hashtags: [list of hashtags from each tweet], \n",
    "                        fulltext: [list of all cleaned/depunkt words across all tweets]\n",
    "                    },\n",
    "                .\n",
    "                .\n",
    "                screen_name_m: \n",
    "                    {\n",
    "                        hashtags: [list of hashtags from each tweet], \n",
    "                        fulltext: [list of all cleaned/depunkt words across all tweets]\n",
    "                    }\n",
    "              }\n",
    "}\n",
    "```\n",
    "\n",
    "Then I can turn this into a pandas dataframe and do some pretty nice data manipulation.\n",
    "\n",
    "We will call this dictionary the `master_dict`.\n",
    "\n",
    "To do this, we first define some helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Defining some utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "C:\\Users\\adity\\AppData\\Local\\Continuum\\anaconda3\\envs\\insightenv\\lib\\site-packages\\smart_open\\smart_open_lib.py:402: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\nC:\\Users\\adity\\AppData\\Local\\Continuum\\anaconda3\\envs\\insightenv\\lib\\site-packages\\smart_open\\smart_open_lib.py:402: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
    }
   ],
   "source": [
    "def get_user(tweet):\n",
    "    \"\"\"\n",
    "    input: tweet dictionary\n",
    "    returns: return the username\n",
    "    \"\"\"\n",
    "    return tweet['user']['screen_name']\n",
    "\n",
    "\n",
    "def get_hashtag_list(tweet):\n",
    "    \"\"\"\n",
    "    input: tweet dictionary\n",
    "    returns: list of all hashtags in both the direct tweet and the\n",
    "    retweet \n",
    "    \"\"\"\n",
    "\n",
    "    l = []\n",
    "    for d in tweet['entities']['hashtags']:\n",
    "        l += [d['text']]\n",
    "\n",
    "    if 'retweeted_status' in tweet.keys():\n",
    "        for d in tweet['retweeted_status']['entities']['hashtags']:\n",
    "            l += [d['text']]\n",
    "    return l\n",
    "\n",
    "\n",
    "def tokenizer_cleaner_nostop_lemmatizer(text):\n",
    "    \"\"\"\n",
    "    This function tokenizes the text of a tweet, cleans it off punctuation,\n",
    "    removes stop words, and lemmatizes the words (i.e. finds word roots to remove noise)\n",
    "    I am largely using the gensim and spacy packages \n",
    "\n",
    "    Input: Some text\n",
    "    Output: List of tokenized, cleaned, lemmatized words\n",
    "    \"\"\"\n",
    "\n",
    "    tokenized_depunkt = gensim.utils.simple_preprocess(text, min_len=4, deacc=True)\n",
    "    tokenized_depunkt_nostop = ([word for word in tokenized_depunkt \n",
    "                                 if (word not in stop_words and word in english_words)])\n",
    "    \n",
    "    # Lemmatizer while also only allowing certain parts of speech.\n",
    "    # See here: https://spacy.io/api/annotation\n",
    "    allowed_pos = ['ADJ', 'ADV', 'NOUN', 'PROPN','VERB']\n",
    "    doc = nlp(' '.join(tokenized_depunkt_nostop))\n",
    "    words_final = [token.lemma_ for token in doc if token.pos_ in allowed_pos]\n",
    "    return words_final\n",
    "\n",
    "    \n",
    "def get_tweet_words_list(tweet):\n",
    "    \"\"\"\n",
    "    This function takes in a tweet and checks if there is a retweet associated with it\n",
    "    input: tweet\n",
    "    output: list of tokenized words without punctuation\n",
    "    \"\"\"\n",
    "\n",
    "    text = tweet['full_text']\n",
    "    clean_words = tokenizer_cleaner_nostop_lemmatizer(text)\n",
    "    \n",
    "    if 'retweeted_status' in tweet.keys():\n",
    "        retweet_text = tweet['retweeted_status']['full_text']\n",
    "        retweet_clean_words = tokenizer_cleaner_nostop_lemmatizer(retweet_text)\n",
    "        clean_words += retweet_clean_words\n",
    "    return clean_words\n",
    "\n",
    "# load the classifer model and the commercial filter LDA model\n",
    "with open('./models/commercial-filter-classifier.model', 'rb') as filehandle:\n",
    "    clf = pickle.load(filehandle)\n",
    "\n",
    "lda_model_path = './ldamodels/random_users/model.model'\n",
    "lda_model_random_users = gensim.models.ldamodel.LdaModel.load(\n",
    "                                                        lda_model_path)\n",
    "\n",
    "with open('./ldamodels/random_users/corpus.corpus', 'rb') as filehandle:\n",
    "    random_users_corpus = pickle.load(filehandle)\n",
    "\n",
    "def get_augmented_feature_vectors(feature_vectors):\n",
    "    \"\"\"\n",
    "    Takes in the feature vector list of list and augments it. gensim does not\n",
    "    actually put a 0 for topics that have 0 probability so I need to manually\n",
    "    add it in to build my feature vector. \n",
    "    input: accepts the feature vectors output by gensim. It is a list of \n",
    "    tuples - one list entry per document and tuple are (topic, probability)\n",
    "    pairs.\n",
    "    returns: Augmented feature vectors as list of list. Each list entry \n",
    "    corresponds to one document, with the i-th element in the inner list\n",
    "    corresponding to the probability that the document was generated with \n",
    "    topic i.\n",
    "    \"\"\"\n",
    "    augmented = []\n",
    "    for i, vector in enumerate(feature_vectors): # each vector is a list of tuples\n",
    "        topics = [tup[0] for tup in vector]\n",
    "        for t in range(10):\n",
    "            if t not in topics:\n",
    "                feature_vectors[i].append((t, 0))\n",
    "        new_feature_vector = sorted(feature_vectors[i], key=lambda tup: tup[0])\n",
    "        augmented.append([tup[1] for tup in new_feature_vector])\n",
    "    return augmented\n",
    "\n",
    "def feature_vector_commercial_model(doc):\n",
    "    \"\"\"\n",
    "    This function accepts a document and then makes an inference with the\n",
    "    lda model trained on the commercial/random user dataset. It then\n",
    "    returns the feature vector which consists of the probabilities that document\n",
    "    was generated by topic i. Note that this feature vector is unaugmented, and we\n",
    "    will call the augmentation function to add zeros appropriately.\n",
    "    \n",
    "    input: document consisting of the full text of all tweets of a particular user\n",
    "    returns: probability feature vector. \n",
    "    \"\"\"\n",
    "    id2word = corpora.Dictionary([doc]) # To satisfy that id2word needs a list of lists\n",
    "    corpus = id2word.doc2bow(doc)\n",
    "    topics = lda_model_random_users.get_document_topics(corpus)\n",
    "    return get_augmented_feature_vectors([topics]) # for consistency in dimensions\n",
    "\n",
    "def commercial_Q(feature_vector):\n",
    "    \"\"\"\n",
    "    This function uses the pre-trained binary classifer to make a prediction on whether\n",
    "    given the full text document of a given user, whether it is commercial or not.\n",
    "\n",
    "    input: feature vector\n",
    "    returns: scalar prediction on commercial label (1 if yes)\n",
    "    \"\"\"\n",
    "\n",
    "    return clf.predict(feature_vector)[0]\n",
    "\n",
    "# The workflow above upto and including commercial_Q all passed some basic unit tests\n",
    "# showing that it functions correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating the `master_dict`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('./data/all_tweets_dict.data', 'rb') as filehandle:\n",
    "#     all_tweets_data = pickle.load(filehandle)\n",
    "\n",
    "# master_dict = {}\n",
    "\n",
    "# for market in all_tweets_data:\n",
    "#     followers = all_tweets_data[market]\n",
    "#     master_dict[market] = {}\n",
    "\n",
    "#     for follower in tqdm(followers):\n",
    "#         tweets = all_tweets_data[market][follower] # list of tweet_.json\n",
    "#         master_dict[market][follower] = {}\n",
    "#         master_dict[market][follower]['hashtags'] = []\n",
    "#         master_dict[market][follower]['fulltext'] = []\n",
    "#         for tweet in tweets:\n",
    "#             hashtags = get_hashtag_list(tweet)\n",
    "#             words = get_tweet_words_list(tweet)\n",
    "            \n",
    "#             master_dict[market][follower]['hashtags'].extend(hashtags)\n",
    "#             master_dict[market][follower]['fulltext'].extend(words)\n",
    "\n",
    "# with open('./data/master_dict.data', 'wb') as filehandle:\n",
    "#     pickle.dump(master_dict, filehandle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the Latent Dirichlet Allocation\n",
    "\n",
    "Now we apply the LDA algorithm to identify themes in the documents/topics. In my case, a single document corresponds to the set of all words of a single user's tweets. Note that the list of words that comprose a document have already been cleaned, tokenized and lemmatized. \n",
    "\n",
    "One other thought is to have one MASSIVE document containing all tweets of all users, and then finding the topics there. In the comparison step, I could use these top topics and then compare this to all tweets of individual users and then returning top-k users based on similarity. See [this](https://stats.stackexchange.com/questions/269031/how-to-find-similar-documents-after-a-latent-dirichlet-allocation-model-is-bui) stack-exchange post for ideas.\n",
    "\n",
    "We first write some helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/master_dict.data', 'rb') as filehandle:\n",
    "    master_dict = pickle.load(filehandle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_docs(d, market):\n",
    "    \"\"\"\n",
    "    Accepts a market and then returns the documents for the market. A document\n",
    "    is a list of of word lists for each user in the market city i.e. it is a list of lists.\n",
    "    Each outer list is a follower and the innner list is the cleaner, tokenized, depunkt, \n",
    "    lematized set of words for that follower.\n",
    "    \"\"\"\n",
    "    docs = []\n",
    "    for user in d[market]:\n",
    "        text_list = d[market][user]['fulltext']\n",
    "        docs.append(text_list)\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Total documents:419, Documents accepted:246\nLength of corpus:246\n"
    }
   ],
   "source": [
    "markets = list(master_dict.keys())\n",
    "market_index = 1\n",
    "docs = get_docs(master_dict, markets[market_index])\n",
    "\n",
    "# use the commercial_Q filter to see if a document needs to be included. \n",
    "# For now, I am using a for loop to keep track of the count of how many documents are\n",
    "# rejected, but can probably change to list comprehension\n",
    "\n",
    "docs_filtered = []\n",
    "for doc in docs:\n",
    "    f = feature_vector_commercial_model(doc)\n",
    "    if commercial_Q(f) == 0: # 0 means it is not a commercial doc\n",
    "        docs_filtered.append(doc)\n",
    "print('Total documents:', len(docs), ', Documents accepted:', len(docs_filtered))\n",
    "id2word = corpora.Dictionary(docs_filtered)\n",
    "\n",
    "# Idea: Keep only those tokens that appear in at least 10% of the documents\n",
    "id2word.filter_extremes(no_below=int(0.1*len(docs_filtered)))\n",
    "corpus = [id2word.doc2bow(doc) for doc in docs_filtered]\n",
    "print('Length of corpus:', len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "time:83.89632606506348\nC:\\Users\\adity\\AppData\\Local\\Continuum\\anaconda3\\envs\\insightenv\\lib\\site-packages\\smart_open\\smart_open_lib.py:402: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\nC:\\Users\\adity\\AppData\\Local\\Continuum\\anaconda3\\envs\\insightenv\\lib\\site-packages\\smart_open\\smart_open_lib.py:402: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n[(0,\n'0.000*\"bullet\" + 0.000*\"bowl\" + 0.000*\"cartoon\" + 0.000*\"captain\" + '\n'0.000*\"bush\" + 0.000*\"ceremony\" + 0.000*\"buddy\" + 0.000*\"blanket\" + '\n'0.000*\"blessing\" + 0.000*\"chris\"'),\n(1,\n'0.029*\"entrepreneur\" + 0.011*\"teacher\" + 0.010*\"tech\" + 0.010*\"success\" + '\n'0.009*\"register\" + 0.008*\"customer\" + 0.007*\"expert\" + 0.007*\"training\" + '\n'0.007*\"technology\" + 0.007*\"client\"'),\n(2,\n'0.026*\"impeachment\" + 0.018*\"trial\" + 0.015*\"republican\" + 0.014*\"iran\" + '\n'0.013*\"democrats\" + 0.012*\"republicans\" + 0.012*\"ukraine\" + 0.010*\"russia\" '\n'+ 0.009*\"impeach\" + 0.008*\"investigation\"'),\n(3,\n'0.021*\"farmer\" + 0.013*\"wine\" + 0.010*\"spice\" + 0.010*\"fresh\" + '\n'0.009*\"taste\" + 0.009*\"beach\" + 0.009*\"cake\" + 0.009*\"square\" + '\n'0.008*\"farm\" + 0.008*\"chicken\"'),\n(4,\n'0.097*\"player\" + 0.095*\"blazer\" + 0.083*\"trailblazer\" + 0.075*\"coach\" + '\n'0.070*\"fan\" + 0.043*\"dragon\" + 0.038*\"playoff\" + 0.035*\"logo\" + '\n'0.027*\"league\" + 0.024*\"basketball\"'),\n(5,\n'0.009*\"unity\" + 0.006*\"immigrant\" + 0.006*\"sexual\" + 0.005*\"mental\" + '\n'0.005*\"solidarity\" + 0.004*\"mass\" + 0.004*\"border\" + 0.004*\"wealth\" + '\n'0.004*\"oregonian\" + 0.004*\"journalist\"'),\n(6,\n'0.013*\"boat\" + 0.013*\"adventure\" + 0.011*\"glass\" + 0.011*\"tour\" + '\n'0.011*\"premiere\" + 0.010*\"coast\" + 0.010*\"amazon\" + 0.009*\"lake\" + '\n'0.009*\"festival\" + 0.009*\"river\"'),\n(7,\n'0.005*\"abortion\" + 0.003*\"mouth\" + 0.003*\"david\" + 0.003*\"dog\" + '\n'0.003*\"copy\" + 0.002*\"patriot\" + 0.002*\"weird\" + 0.002*\"sister\" + '\n'0.002*\"boy\" + 0.002*\"teacher\"'),\n(8,\n'0.062*\"recipe\" + 0.052*\"pain\" + 0.042*\"cannabis\" + 0.041*\"patient\" + '\n'0.036*\"print\" + 0.020*\"yoga\" + 0.018*\"marijuana\" + 0.016*\"vegan\" + '\n'0.015*\"identity\" + 0.014*\"healthy\"'),\n(9,\n'0.060*\"vancouver\" + 0.037*\"officer\" + 0.030*\"bridge\" + 0.028*\"rain\" + '\n'0.027*\"downtown\" + 0.027*\"flood\" + 0.027*\"bike\" + 0.026*\"traffic\" + '\n'0.025*\"council\" + 0.023*\"waterfront\"')]\n"
    }
   ],
   "source": [
    "def compute_lda(corpus, id2word, k=10, alpha='auto'):\n",
    "    \"\"\"\n",
    "    Performs the LDA and returns the computer model.\n",
    "    Input: Corpus, dictionary and hyperparameters to optimize\n",
    "    Output: the fitted/computed LDA model\n",
    "    \"\"\"\n",
    "    lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus, \n",
    "                                                id2word=id2word,\n",
    "                                                num_topics=k,\n",
    "                                                random_state=100,\n",
    "                                                update_every=1,\n",
    "                                                chunksize=5,\n",
    "                                                passes=100,\n",
    "                                                alpha=.001,\n",
    "                                                iterations=100,\n",
    "                                                per_word_topics=True)\n",
    "    return lda_model\n",
    "t1 = time.time()\n",
    "lda_model = compute_lda(corpus, id2word)\n",
    "t2 = time.time()\n",
    "print('time:', t2-t1)\n",
    "# save the model\n",
    "filename_model = './ldamodels/market' + str(market_index) + '/model.model'\n",
    "lda_model.save(filename_model)\n",
    "# save the corpus\n",
    "filename_corpus = './ldamodels/market' + str(market_index) + '/corpus.corpus'\n",
    "with open(filename_corpus, 'wb') as filehandle:\n",
    "    pickle.dump(corpus, filehandle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "pprint(lda_model.print_topics())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some comments on the hyperparameter tuning:\n",
    "\n",
    "1. Doing a chunksize of 1 is pretty slow and time consuming (although it might be worthwhile to time this more accurately). I think either choosing `chunksize=5` or `chunksize=10` works well.\n",
    "2. `passes` is a parameter similar to number of epochs. \n",
    "3. `alpha='auto'` seems to work pretty well. \n",
    "4. Keep `random_state=100` in case you want to repeat results. \n",
    "5. Keep `update_every` small, ideally equal to 1. \n",
    "6. Use a for loop to calculate the optimal number of topics. This just has to be done. Might be better to do this in the background on the python terminal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating the optimal number of opics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "\n  0%|          | 0/15 [00:00<?, ?it/s](5, 0.5189726908534633)\n\n  7%|▋         | 1/15 [01:20<18:41, 80.13s/it](6, 0.519356341248931)\n\n 13%|█▎        | 2/15 [03:00<18:41, 86.30s/it](7, 0.47789018284809354)\n\n 20%|██        | 3/15 [04:48<18:31, 92.60s/it](8, 0.469150491505505)\n\n 27%|██▋       | 4/15 [06:41<18:05, 98.72s/it](9, 0.48546409778958455)\n\n 33%|███▎      | 5/15 [08:35<17:13, 103.40s/it](10, 0.4606212152153395)\n\n 40%|████      | 6/15 [10:21<15:38, 104.23s/it](11, 0.4329650553357137)\n\n 47%|████▋     | 7/15 [12:06<13:56, 104.55s/it](12, 0.43535675757035736)\n\n 53%|█████▎    | 8/15 [13:41<11:51, 101.62s/it](13, 0.4217650294141849)\n\n 60%|██████    | 9/15 [15:24<10:11, 101.89s/it](14, 0.4541297801261875)\n\n 67%|██████▋   | 10/15 [17:14<08:42, 104.42s/it](15, 0.4334219414278646)\n\n 73%|███████▎  | 11/15 [19:12<07:13, 108.42s/it](16, 0.4723659369842674)\n\n 80%|████████  | 12/15 [48:11<29:52, 597.64s/it](17, 0.4893456672625304)\n\n 87%|████████▋ | 13/15 [50:04<15:04, 452.33s/it](18, 0.47428237921676214)\n\n 93%|█████████▎| 14/15 [52:10<05:54, 354.40s/it](19, 0.43677098213132115)\n\n100%|██████████| 15/15 [54:23<00:00, 288.09s/it]"
    },
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (10,) and (15, 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-3b79c4c951b7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mcoherence_scores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptimal_topics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcoherence_scores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\insightenv\\lib\\site-packages\\matplotlib\\pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[1;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2787\u001b[0m     return gca().plot(\n\u001b[0;32m   2788\u001b[0m         *args, scalex=scalex, scaley=scaley, **({\"data\": data} if data\n\u001b[1;32m-> 2789\u001b[1;33m         is not None else {}), **kwargs)\n\u001b[0m\u001b[0;32m   2790\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2791\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\insightenv\\lib\\site-packages\\matplotlib\\axes\\_axes.py\u001b[0m in \u001b[0;36mplot\u001b[1;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1663\u001b[0m         \"\"\"\n\u001b[0;32m   1664\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmlines\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLine2D\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_alias_map\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1665\u001b[1;33m         \u001b[0mlines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1666\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1667\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\insightenv\\lib\\site-packages\\matplotlib\\axes\\_base.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    223\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m                 \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 225\u001b[1;33m             \u001b[1;32myield\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_plot_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    226\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_next_color\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\insightenv\\lib\\site-packages\\matplotlib\\axes\\_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[1;34m(self, tup, kwargs)\u001b[0m\n\u001b[0;32m    389\u001b[0m             \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindex_of\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    390\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 391\u001b[1;33m         \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_xy_from_xy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    392\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    393\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommand\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'plot'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\insightenv\\lib\\site-packages\\matplotlib\\axes\\_base.py\u001b[0m in \u001b[0;36m_xy_from_xy\u001b[1;34m(self, x, y)\u001b[0m\n\u001b[0;32m    268\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    269\u001b[0m             raise ValueError(\"x and y must have same first dimension, but \"\n\u001b[1;32m--> 270\u001b[1;33m                              \"have shapes {} and {}\".format(x.shape, y.shape))\n\u001b[0m\u001b[0;32m    271\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    272\u001b[0m             raise ValueError(\"x and y can be no greater than 2-D, but have \"\n",
      "\u001b[1;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (10,) and (15, 2)"
     ]
    }
   ],
   "source": [
    "def optimal_topics():\n",
    "    coherence_scores = []\n",
    "    for k in tqdm(range(5, 20)):\n",
    "        lda_model = compute_lda(corpus, id2word, k=k)\n",
    "        coherence_model_lda = CoherenceModel(model=lda_model,\n",
    "                                        texts=docs_filtered,\n",
    "                                        dictionary=id2word,\n",
    "                                        coherence='c_v')\n",
    "        coherence_lda = coherence_model_lda.get_coherence()\n",
    "        print((k, coherence_lda))\n",
    "        coherence_scores.append((k, coherence_lda))\n",
    "    return coherence_scores\n",
    "    \n",
    "coherence_scores = optimal_topics()\n",
    "plt.plot(range(5,15), coherence_scores)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "C:\\Users\\adity\\AppData\\Local\\Continuum\\anaconda3\\envs\\insightenv\\lib\\site-packages\\pyLDAvis\\_prepare.py:257: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\nof pandas will change to not sort by default.\n\nTo accept the future behavior, pass 'sort=False'.\n\nTo retain the current behavior and silence the warning, pass 'sort=True'.\n\n  return pd.concat([default_term_info] + list(topic_dfs))\n"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-75-68dfd80b0ffd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mt1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mLDAvis_prepared\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprepare\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlda_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlda_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mid2word\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'tsne'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mt2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'LDAvis prep time:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt2\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mt1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLDAvis_prepared\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\insightenv\\lib\\site-packages\\pyLDAvis\\gensim.py\u001b[0m in \u001b[0;36mprepare\u001b[1;34m(topic_model, corpus, dictionary, doc_topic_dist, **kwargs)\u001b[0m\n\u001b[0;32m    117\u001b[0m     \"\"\"\n\u001b[0;32m    118\u001b[0m     \u001b[0mopts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_extract_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtopic_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdoc_topic_dist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 119\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mvis_prepare\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mopts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\insightenv\\lib\\site-packages\\pyLDAvis\\_prepare.py\u001b[0m in \u001b[0;36mprepare\u001b[1;34m(topic_term_dists, doc_topic_dists, doc_lengths, vocab, term_frequency, R, lambda_step, mds, n_jobs, plot_opts, sort_topics)\u001b[0m\n\u001b[0;32m    397\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    398\u001b[0m    \u001b[0mtopic_info\u001b[0m         \u001b[1;33m=\u001b[0m \u001b[0m_topic_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtopic_term_dists\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtopic_proportion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mterm_frequency\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mterm_topic_freq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlambda_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mR\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 399\u001b[1;33m    \u001b[0mtoken_table\u001b[0m        \u001b[1;33m=\u001b[0m \u001b[0m_token_table\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtopic_info\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mterm_topic_freq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mterm_frequency\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    400\u001b[0m    \u001b[0mtopic_coordinates\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_topic_coordinates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtopic_term_dists\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtopic_proportion\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    401\u001b[0m    \u001b[0mclient_topic_order\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtopic_order\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\insightenv\\lib\\site-packages\\pyLDAvis\\_prepare.py\u001b[0m in \u001b[0;36m_token_table\u001b[1;34m(topic_info, term_topic_freq, vocab, term_frequency)\u001b[0m\n\u001b[0;32m    279\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    280\u001b[0m    \u001b[0mtoken_table\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Freq'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtoken_table\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Freq'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 281\u001b[1;33m    \u001b[0mtoken_table\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Term'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvocab\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtoken_table\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    282\u001b[0m    \u001b[1;31m# Normalize token frequencies:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    283\u001b[0m    \u001b[0mtoken_table\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Freq'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtoken_table\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFreq\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mterm_frequency\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtoken_table\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\insightenv\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mvalues\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   3606\u001b[0m     \u001b[1;31m# Uncategorized Methods\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3607\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3608\u001b[1;33m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3609\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3610\u001b[0m         \"\"\"\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "t1 = time.time()\n",
    "LDAvis_prepared = pyLDAvis.gensim.prepare(lda_model, corpus, dictionary=lda_model.id2word, mds='tsne')\n",
    "t2 = time.time()\n",
    "print('LDAvis prep time:', t2-t1)\n",
    "pyLDAvis.show(LDAvis_prepared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3.6.8 64-bit ('insightenv': conda)",
   "language": "python",
   "name": "python36864bitinsightenvconda4d948c2035f94dc3b09753f05f123a75"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8-final"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}