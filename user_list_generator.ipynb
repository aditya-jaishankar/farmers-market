{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.6.8-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python36864bitinsightenvconda4d948c2035f94dc3b09753f05f123a75",
   "display_name": "Python 3.6.8 64-bit ('insightenv': conda)"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User list generator\n",
    "\n",
    "In this notebook, we find a set of 1000 users whose tweets we want to download. For this, the strategy I am going to use is the fact that people who follow the twitter accounts of well-established farmers markets are vry likely to participate in the idea of farmers markets, are interested in buying local and are socially conscious. \n",
    "\n",
    "The proceeds as follows: I pick a farmers market twitter account at random from listings of the top farmers markets in the US. I then get a list of all their followers and then pick a follower at random (this randomizes features across cities and help remove any location or position specific biases). I repeat this random choice process a thousand times (or two thousand times, depending on how many total tweets I can retrieve - or possibly rank order by number of tweets to find the most prolific users) to build my corpus of tweets. "
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Basic Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import glob\n",
    "import pickle\n",
    "import collections\n",
    "import random\n",
    "from tqdm import tqdm as tqdm\n",
    "\n",
    "import os\n",
    "dirpath = os.path.dirname(os.path.realpath('__file__'))\n",
    "\n",
    "import tweepy\n",
    "import config\n",
    "\n",
    "import nltk\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "XLRDError",
     "evalue": "Unsupported format, or corrupt file: Expected BOF record; found b'config.p'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mXLRDError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-0e671496b516>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlist_markets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./list_of_farmers_markets.xlsx'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msheet_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Selected'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mlist_markets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist_markets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mby\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Num_Followers'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mascending\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mlist_markets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist_markets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mlist_markets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\insightenv\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    186\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 188\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    189\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    190\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0m_deprecate_kwarg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\insightenv\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    186\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 188\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    189\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    190\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0m_deprecate_kwarg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\insightenv\\lib\\site-packages\\pandas\\io\\excel.py\u001b[0m in \u001b[0;36mread_excel\u001b[1;34m(io, sheet_name, header, names, index_col, parse_cols, usecols, squeeze, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, verbose, parse_dates, date_parser, thousands, comment, skip_footer, skipfooter, convert_float, mangle_dupe_cols, **kwds)\u001b[0m\n\u001b[0;32m    348\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    349\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 350\u001b[1;33m         \u001b[0mio\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    351\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    352\u001b[0m     return io.parse(\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\insightenv\\lib\\site-packages\\pandas\\io\\excel.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, io, engine)\u001b[0m\n\u001b[0;32m    651\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_io\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_stringify_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    652\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 653\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engines\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_io\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    654\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    655\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__fspath__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\insightenv\\lib\\site-packages\\pandas\\io\\excel.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, filepath_or_buffer)\u001b[0m\n\u001b[0;32m    422\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbook\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxlrd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen_workbook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_contents\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    423\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 424\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbook\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxlrd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen_workbook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    425\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    426\u001b[0m             raise ValueError('Must explicitly set engine if not passing in'\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\insightenv\\lib\\site-packages\\xlrd\\__init__.py\u001b[0m in \u001b[0;36mopen_workbook\u001b[1;34m(filename, logfile, verbosity, use_mmap, file_contents, encoding_override, formatting_info, on_demand, ragged_rows)\u001b[0m\n\u001b[0;32m    155\u001b[0m         \u001b[0mformatting_info\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mformatting_info\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m         \u001b[0mon_demand\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mon_demand\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 157\u001b[1;33m         \u001b[0mragged_rows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mragged_rows\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    158\u001b[0m     )\n\u001b[0;32m    159\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mbk\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\insightenv\\lib\\site-packages\\xlrd\\book.py\u001b[0m in \u001b[0;36mopen_workbook_xls\u001b[1;34m(filename, logfile, verbosity, use_mmap, file_contents, encoding_override, formatting_info, on_demand, ragged_rows)\u001b[0m\n\u001b[0;32m     90\u001b[0m         \u001b[0mt1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mperf_counter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m         \u001b[0mbk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_time_stage_1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mt1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mt0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 92\u001b[1;33m         \u001b[0mbiff_version\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetbof\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXL_WORKBOOK_GLOBALS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     93\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mbiff_version\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mXLRDError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Can't determine file's BIFF version\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\insightenv\\lib\\site-packages\\xlrd\\book.py\u001b[0m in \u001b[0;36mgetbof\u001b[1;34m(self, rqd_stream)\u001b[0m\n\u001b[0;32m   1276\u001b[0m             \u001b[0mbof_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Expected BOF record; met end of file'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1277\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mopcode\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbofcodes\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1278\u001b[1;33m             \u001b[0mbof_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Expected BOF record; found %r'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmem\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msavpos\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0msavpos\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1279\u001b[0m         \u001b[0mlength\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget2bytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1280\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlength\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mMY_EOF\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\insightenv\\lib\\site-packages\\xlrd\\book.py\u001b[0m in \u001b[0;36mbof_error\u001b[1;34m(msg)\u001b[0m\n\u001b[0;32m   1270\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1271\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mbof_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1272\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mXLRDError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Unsupported format, or corrupt file: '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1273\u001b[0m         \u001b[0msavpos\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_position\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1274\u001b[0m         \u001b[0mopcode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget2bytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mXLRDError\u001b[0m: Unsupported format, or corrupt file: Expected BOF record; found b'config.p'"
     ]
    }
   ],
   "source": [
    "list_markets = pd.read_excel('./list_of_farmers_markets.xlsx', sheet_name='Selected')\n",
    "list_markets = list_markets.sort_values(by=['Num_Followers'], ascending=False)\n",
    "list_markets = list_markets.reset_index(drop=True)\n",
    "list_markets.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the number of people to pick from each city; for now I am choosing\n",
    "# this number as being proportional to the total number of follower\n",
    "list_markets['Num_Followers_ToPick'] = (list_markets['Num_Followers']/\n",
    "                                        np.sum(list_markets['Num_Followers'])*2000).astype(np.int)\n",
    "list_markets.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Authenticating the Twitter API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer_key = config.consumer_key\n",
    "consumer_secret = config.consumer_secret\n",
    "auth = tweepy.AppAuthHandler(consumer_key, consumer_secret)\n",
    "\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "1994"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Downloading the user handles i.e. `screen_name` of users\n",
    "\n",
    "First, we begin with some helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "We iterature through the list of screen_names and we download all other follower_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In future runs, if you don't have to download this data again,\n",
    "# just load the original pickle file\n",
    "# followers_dict = {}\n",
    "# for market in tqdm(list_markets['Twitter_handle']):\n",
    "#     try:\n",
    "#         followers = tweepy.Cursor(api.followers,\n",
    "#                                     screen_name=market,\n",
    "#                                     lang='en',\n",
    "#                                     include_entities=True,\n",
    "#                                     count=2000).items(2000)\n",
    "#         followers_list = list(followers)\n",
    "#         followers_json = list(map(lambda f: f._json, followers_list))\n",
    "#         followers_dict[market] = followers_dict.get(market, []) + followers_json\n",
    "#     except:\n",
    "#         with open('./followers_dict.data', 'wb') as filehandle:\n",
    "#             pickle.dump(followers_dict, filehandle)\n",
    "\n",
    "# with open('./followers_dict.data', 'wb') as filehandle:\n",
    "#     pickle.dump(followers_dict, filehandle)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I now have a dictionary of the form\n",
    "\n",
    "```\n",
    "{\n",
    "    market1: [{user1_json}, {user2_json}, ..., {user2000_json}],\n",
    "    market2: [{user1_json}, {user2_json}, ..., {user2000_json}],\n",
    "    .\n",
    "    .\n",
    "    market10: [{user1_json}, {user2_json}, ..., {user2000_json}],    \n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Trimming `follower_dict`\n",
    "\n",
    "Now that I have the `follower_dict`, I have a lot of users along with all their metadata. To really distinguish between users who provide signal and users who provide noise, I choose two parameters: users who have more than 500 followers themselves, and users who have tweeted out more than 500 times (relax this second condition if 500 is too high - I don't have a sense for how high this should be). I have 10 x 1000 total followers so hopefully I will find enough users with over 500 followers and over 500 tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Defining the function that selects followers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selected_follower(follower):\n",
    "    \"\"\"\n",
    "    input: Accepts a follower json and then checkes to see if they have over 500 followers and have tweeted over 500 times.\n",
    "    returns: Boolean if criteria are met \n",
    "    \"\"\"\n",
    "    followers_bool = False\n",
    "    tweets_bool = False\n",
    "    if follower['followers_count'] >= 300:\n",
    "        followers_bool = True\n",
    "    if follower['statuses_count'] >= 1000:\n",
    "        tweets_bool = True\n",
    "    return followers_bool and tweets_bool"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###  Making the `master_dict` only with the selected followers\n",
    "\n",
    "Use only if downloading new data. Otherwise, go ahead and use the file that has been exported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "100%|██████████| 10/10 [00:00<00:00, 556.98it/s]\n4323\n"
    }
   ],
   "source": [
    "with open('./followers_dict.data', 'rb') as filehandle:\n",
    "    followers_dict = pickle.load(filehandle)\n",
    "\n",
    "counter = 0\n",
    "follower_dict_trimmed = collections.defaultdict(lambda: [])\n",
    "for market in tqdm(followers_dict):\n",
    "    followers = followers_dict[market]\n",
    "    for follower in followers:\n",
    "        if selected_follower(follower):\n",
    "            counter += 1\n",
    "            follower_dict_trimmed[market] = follower_dict_trimmed[market] + [follower]\n",
    "print(counter)\n",
    "\n",
    "# With these criterion, I get 2028 unique followers. I next download 500 tweets from each one of those 2028 followers. Perhaps this will give me enough diversity and a large enough corpus of words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./followers_dict_trimmed.data', 'wb') as filehandle:\n",
    "    pickle.dump(dict(follower_dict_trimmed), filehandle)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "collections.defaultdict"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Downloading 500 tweets from each of the selected followers\n",
    "\n",
    "I am going to retain the market split because I want documents grouped by market. i.e. I am looking for a dictionary of the following structure:\n",
    "\n",
    "```\n",
    "{\n",
    "    market1: {\n",
    "                user1: [{tweet1_json}, {tweet2_json}, ..., {tweetn_json}],\n",
    "                user2: [{tweet1_json}, {tweet2_json}, ..., {tweetn_json}],\n",
    "                .\n",
    "                .\n",
    "                usern: [{tweet1_json}, {tweet2_json}, ..., {tweetn_json}]\n",
    "            }\n",
    "    .\n",
    "    .\n",
    "    market10: {\n",
    "                user1: [{tweet1_json}, {tweet2_json}, ..., {tweetn_json}],\n",
    "                user2: [{tweet1_json}, {tweet2_json}, ..., {tweetn_json}],\n",
    "                .\n",
    "                .\n",
    "                usern: [{tweet1_json}, {tweet2_json}, ..., {tweetn_json}]\n",
    "              }\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "0%|          | 0/3 [00:00<?, ?it/s]Rate limit reached. Sleeping for: 64\n100%|██████████| 3/3 [39:37<00:00, 845.21s/it]\n"
    }
   ],
   "source": [
    "with open('./followers_dict_trimmed.data', 'rb') as filehandle:\n",
    "    follower_dict_trimmed = pickle.load(filehandle)\n",
    "\n",
    "all_tweets = {}\n",
    "markets = list(follower_dict_trimmed.keys())\n",
    "for market in tqdm(markets[:3]):\n",
    "    all_tweets[market] = {}\n",
    "    followers = follower_dict_trimmed[market]\n",
    "    for follower in followers:\n",
    "        try:\n",
    "            screen_name = follower['screen_name']\n",
    "            tweets = tweepy.Cursor(api.user_timeline,\n",
    "                                    screen_name=screen_name,\n",
    "                                    tweet_mode='extended',\n",
    "                                    count=500).items(500)\n",
    "            for tweet in tweets:\n",
    "                all_tweets[market][screen_name] = all_tweets[market].get(screen_name, []) + [tweet._json]\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "# with open('./all_tweets_dict.data', 'wb') as filehandle:\n",
    "#     pickle.dump(all_tweets, filehandle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./all_tweets_dict.data', 'wb') as filehandle:\n",
    "    pickle.dump(all_tweets, filehandle)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Note on the structure of the output dictionary: It has the form\n",
    "\n",
    "```\n",
    "{\n",
    "    market1: {\n",
    "                screen_name1: [{tweet1, ..., tweet500}],\n",
    "                .\n",
    "                .\n",
    "                .\n",
    "                screen_name2028: [{tweet1, ..., tweet500}]\n",
    "            }\n",
    "    .\n",
    "    .\n",
    "    .\n",
    "    market2: {\n",
    "                screen_name1: [{tweet1, ..., tweet500}],\n",
    "                .\n",
    "                .\n",
    "                .\n",
    "                screen_name2028: [{tweet1, ..., tweet500}]\n",
    "             }\n",
    "}\n",
    "```\n",
    "\n",
    "Now that I have 500 tweets from each of the 2028 users, I can use my previous LDA code to make a the requisite dictionary of the form:\n",
    "\n",
    "```\n",
    "{\n",
    "    user1: \n",
    "        {\n",
    "            hashtags: [list of hashtags from each tweet], \n",
    "            fulltext: [list of all cleaned/depunkt words across all tweets]\n",
    "        },\n",
    "    .\n",
    "    .\n",
    "    .\n",
    "    usern: \n",
    "        {\n",
    "            hashtags: [list of hashtags from each tweet], \n",
    "            fulltext: [list of all cleaned/depunkt words across all tweets]\n",
    "        }\n",
    "}\n",
    "```"
   ]
  }
 ]
}