{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.6.8-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python36864bitinsightenvconda4d948c2035f94dc3b09753f05f123a75",
   "display_name": "Python 3.6.8 64-bit ('insightenv': conda)"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User list generator\n",
    "\n",
    "In this notebook, we find a set of 1000 users whose tweets we want to download. For this, the strategy I am going to use is the fact that people who follow the twitter accounts of well-established farmers markets are vry likely to participate in the idea of farmers markets, are interested in buying local and are socially conscious. \n",
    "\n",
    "The proceeds as follows: I pick a farmers market twitter account at random from listings of the top farmers markets in the US. I then get a list of all their followers and then pick a follower at random (this randomizes features across cities and help remove any location or position specific biases). I repeat this random choice process a thousand times (or two thousand times, depending on how many total tweets I can retrieve - or possibly rank order by number of tweets to find the most prolific users) to build my corpus of tweets. "
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Basic Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import glob\n",
    "import pickle\n",
    "import collections\n",
    "import random\n",
    "from tqdm import tqdm as tqdm\n",
    "\n",
    "import os\n",
    "dirpath = os.path.dirname(os.path.realpath('__file__'))\n",
    "\n",
    "import tweepy\n",
    "import config\n",
    "\n",
    "import nltk\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Location</th>\n      <th>Twitter Tag</th>\n      <th>Num_Followers</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Philadelphia, PA</td>\n      <td>@thefoodtrust</td>\n      <td>37000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>New York, NY</td>\n      <td>@unsqgreenmarket</td>\n      <td>25500</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Chicago, IL</td>\n      <td>@greencitymarket</td>\n      <td>24600</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Seattle, WA</td>\n      <td>@seattleFarmMkts</td>\n      <td>17100</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Santa Monica, CA</td>\n      <td>@smfms</td>\n      <td>8291</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Des Moines, IA</td>\n      <td>@DTFarmersMarket</td>\n      <td>7391</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>Austin, TX</td>\n      <td>@SFClocal</td>\n      <td>5498</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "           Location       Twitter Tag  Num_Followers\n0  Philadelphia, PA     @thefoodtrust          37000\n1      New York, NY  @unsqgreenmarket          25500\n2       Chicago, IL  @greencitymarket          24600\n3       Seattle, WA  @seattleFarmMkts          17100\n4  Santa Monica, CA            @smfms           8291\n5    Des Moines, IA  @DTFarmersMarket           7391\n6        Austin, TX         @SFClocal           5498"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_markets = pd.read_excel('./list_of_farmers_markets.xlsx')\n",
    "list_markets = list_markets.sort_values(by=['Num_Followers'], ascending=False)\n",
    "list_markets = list_markets.reset_index(drop=True)\n",
    "\n",
    "# Based on some data inspection, we remove portland and remove GrowNYC. \n",
    "list_markets = list_markets.drop([1, 4, 9]).reset_index(drop=True)\n",
    "list_markets.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Authenticating the Twitter API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer_key = config.consumer_key\n",
    "consumer_secret = config.consumer_secret\n",
    "auth = tweepy.AppAuthHandler(consumer_key, consumer_secret)\n",
    "\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "1994"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Downloading the user handles i.e. `screen_name` of users"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "We iterate through the list of screen_names and we download all other follower_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In future runs, if you don't have to download this data again,\n",
    "# just load the original pickle file\n",
    "# followers_dict = {}\n",
    "# for market in tqdm(list_markets['Twitter_handle']):\n",
    "#     try:\n",
    "#         followers = tweepy.Cursor(api.followers,\n",
    "#                                     screen_name=market,\n",
    "#                                     lang='en',\n",
    "#                                     include_entities=True,\n",
    "#                                     count=2000).items(2000)\n",
    "#         followers_list = list(followers)\n",
    "#         followers_json = list(map(lambda f: f._json, followers_list))\n",
    "#         followers_dict[market] = followers_dict.get(market, []) + followers_json\n",
    "#     except:\n",
    "#         with open('./followers_dict.data', 'wb') as filehandle:\n",
    "#             pickle.dump(followers_dict, filehandle)\n",
    "\n",
    "# with open('./followers_dict.data', 'wb') as filehandle:\n",
    "#     pickle.dump(followers_dict, filehandle)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I now have a dictionary of the form\n",
    "\n",
    "```\n",
    "{\n",
    "    market1: [{user1_json}, {user2_json}, ..., {user2000_json}],\n",
    "    market2: [{user1_json}, {user2_json}, ..., {user2000_json}],\n",
    "    .\n",
    "    .\n",
    "    market10: [{user1_json}, {user2_json}, ..., {user2000_json}],    \n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Trimming `follower_dict`\n",
    "\n",
    "Now that I have the `follower_dict`, I have a lot of users along with all their metadata. To really distinguish between users who provide signal and users who provide noise, I choose two parameters: users who have more than 500 followers themselves, and users who have tweeted out more than 500 times (relax this second condition if 500 is too high - I don't have a sense for how high this should be). I have 10 x 1000 total followers so hopefully I will find enough users with over 500 followers and over 500 tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Defining the function that selects followers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selected_follower(follower):\n",
    "    \"\"\"\n",
    "    input: Accepts a follower json and then checkes to see if they have over 500 followers and have tweeted over 500 times.\n",
    "    returns: Boolean if criteria are met \n",
    "    \"\"\"\n",
    "    followers_bool = False\n",
    "    tweets_bool = False\n",
    "    if follower['followers_count'] >= 150:\n",
    "        followers_bool = True\n",
    "    if follower['statuses_count'] >= 500:\n",
    "        tweets_bool = True\n",
    "    return followers_bool and tweets_bool"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###  Making the `master_dict` only with the selected followers\n",
    "\n",
    "Use only if downloading new data. Otherwise, go ahead and use the file that has been exported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "100%|██████████| 10/10 [00:00<00:00, 371.38it/s]\n6665\n"
    }
   ],
   "source": [
    "with open('./data/followers_dict.data', 'rb') as filehandle:\n",
    "    followers_dict = pickle.load(filehandle)\n",
    "\n",
    "counter = 0\n",
    "follower_dict_trimmed = collections.defaultdict(lambda: [])\n",
    "for market in tqdm(followers_dict):\n",
    "    followers = followers_dict[market]\n",
    "    for follower in followers:\n",
    "        if selected_follower(follower):\n",
    "            counter += 1\n",
    "            follower_dict_trimmed[market] = follower_dict_trimmed[market] + [follower]\n",
    "print(counter)\n",
    "# With these criterion, I get 6665 unique followers. I next download 500 tweets from each one of those 6665 followers. Perhaps this will give me enough diversity and a large enough corpus of words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/followers_dict_trimmed.data', 'wb') as filehandle:\n",
    "    pickle.dump(dict(follower_dict_trimmed), filehandle)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "collections.defaultdict"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Downloading 500 tweets from each of the selected followers\n",
    "\n",
    "I am going to retain the market split because I want documents grouped by market. i.e. I am looking for a dictionary of the following structure:\n",
    "\n",
    "```\n",
    "{\n",
    "    market1: {\n",
    "                user1: [{tweet1_json}, {tweet2_json}, ..., {tweetn_json}],\n",
    "                user2: [{tweet1_json}, {tweet2_json}, ..., {tweetn_json}],\n",
    "                .\n",
    "                .\n",
    "                usern: [{tweet1_json}, {tweet2_json}, ..., {tweetn_json}]\n",
    "            }\n",
    "    .\n",
    "    .\n",
    "    market10: {\n",
    "                user1: [{tweet1_json}, {tweet2_json}, ..., {tweetn_json}],\n",
    "                user2: [{tweet1_json}, {tweet2_json}, ..., {tweetn_json}],\n",
    "                .\n",
    "                .\n",
    "                usern: [{tweet1_json}, {tweet2_json}, ..., {tweetn_json}]\n",
    "              }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "100%|██████████| 703/703 [24:22<00:00,  1.86s/it]\n100%|██████████| 723/723 [24:17<00:00,  1.77s/it]\n100%|██████████| 664/664 [35:33<00:00,  1.66s/it]\n100%|██████████| 466/466 [26:05<00:00,  1.89s/it]\n100%|██████████| 710/710 [39:23<00:00,  1.84s/it]\n100%|██████████| 600/600 [38:59<00:00,  1.78s/it]\n100%|██████████| 821/821 [53:41<00:00,  1.82s/it]\n"
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-c79bd13b3b7d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./data/all_tweets_dict.data'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfilehandle\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m     \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_tweets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilehandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mHIGHEST_PROTOCOL\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with open('./data/followers_dict_trimmed.data', 'rb') as filehandle:\n",
    "    follower_dict_trimmed = pickle.load(filehandle)\n",
    "\n",
    "all_tweets = {}\n",
    "markets = list_markets['Twitter Tag']\n",
    "# Only look at the six selected markets\n",
    "\n",
    "# Load all_tweets_dict.data if needed and just add the new market keys.\n",
    "for market in markets:\n",
    "    all_tweets[market] = {}\n",
    "    followers = follower_dict_trimmed[market]\n",
    "    for follower in tqdm(followers):\n",
    "        try:\n",
    "            screen_name = follower['screen_name']\n",
    "            tweets = tweepy.Cursor(api.user_timeline,\n",
    "                                    screen_name=screen_name,\n",
    "                                    tweet_mode='extended',\n",
    "                                    count=500).items(500)\n",
    "            for tweet in tweets:\n",
    "                all_tweets[market][screen_name] = all_tweets[market].get(screen_name, []) + [tweet._json]\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "with open('./data/all_tweets_dict.data', 'wb') as filehandle:\n",
    "    pickle.dump(all_tweets, filehandle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "100%|██████████| 7/7 [10:16<00:00, 88.38s/it]\n"
    }
   ],
   "source": [
    "for market in tqdm(markets):\n",
    "    filename = './data/all_tweets_dict_' + market + '.data'\n",
    "\n",
    "    with open(filename, 'wb') as filehandle:\n",
    "        pickle.dump(all_tweets[market], filehandle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Note on the structure of the output dictionary: It has the form\n",
    "\n",
    "```\n",
    "{\n",
    "    market1: {\n",
    "                screen_name_1: [{tweet1, ..., tweetn}],\n",
    "                .\n",
    "                .\n",
    "                .\n",
    "                screen_name_m: [{tweet1, ..., tweetn}]\n",
    "            }\n",
    "    .\n",
    "    .\n",
    "    .\n",
    "    market10: {\n",
    "                screen_name_1: [{tweet1, ..., tweetn}],\n",
    "                .\n",
    "                .\n",
    "                .\n",
    "                screen_name_2028: [{tweet1, ..., tweetn}]\n",
    "             }\n",
    "}\n",
    "```"
   ]
  }
 ]
}