{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.6.8-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python36864bitinsightenvconda4d948c2035f94dc3b09753f05f123a75",
   "display_name": "Python 3.6.8 64-bit ('insightenv': conda)"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User list generator\n",
    "\n",
    "In this notebook, we find a set of 1000 users whose tweets we want to download. For this, the strategy I am going to use is the fact that people who follow the twitter accounts of well-established farmers markets are vry likely to participate in the idea of farmers markets, are interested in buying local and are socially conscious. \n",
    "\n",
    "The proceeds as follows: I pick a farmers market twitter account at random from listings of the top farmers markets in the US. I then get a list of all their followers and then pick a follower at random (this randomizes features across cities and help remove any location or position specific biases). I repeat this random choice process a thousand times (or two thousand times, depending on how many total tweets I can retrieve - or possibly rank order by number of tweets to find the most prolific users) to build my corpus of tweets. "
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Basic Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import glob\n",
    "import pickle\n",
    "import collections\n",
    "import random\n",
    "from tqdm import tqdm as tqdm\n",
    "\n",
    "import os\n",
    "dirpath = os.path.dirname(os.path.realpath('__file__'))\n",
    "\n",
    "import tweepy\n",
    "import config\n",
    "\n",
    "import nltk\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Location</th>\n      <th>Twitter Tag</th>\n      <th>Num_Followers</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Philadelphia, PA</td>\n      <td>@thefoodtrust</td>\n      <td>37000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Portland, OR</td>\n      <td>@portlandfarmers</td>\n      <td>27500</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>New York, NY</td>\n      <td>@unsqgreenmarket</td>\n      <td>25500</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Chicago, IL</td>\n      <td>@greencitymarket</td>\n      <td>24600</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>New York, NY</td>\n      <td>@GrowNYC</td>\n      <td>19800</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Seattle, WA</td>\n      <td>@seattleFarmMkts</td>\n      <td>17100</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>Santa Monica, CA</td>\n      <td>@smfms</td>\n      <td>8291</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>Des Moines, IA</td>\n      <td>@DTFarmersMarket</td>\n      <td>7391</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>Austin, TX</td>\n      <td>@SFClocal</td>\n      <td>5498</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>Madison, WI</td>\n      <td>@DaneCoFM</td>\n      <td>2805</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "           Location       Twitter Tag  Num_Followers\n0  Philadelphia, PA     @thefoodtrust          37000\n1      Portland, OR  @portlandfarmers          27500\n2      New York, NY  @unsqgreenmarket          25500\n3       Chicago, IL  @greencitymarket          24600\n4      New York, NY          @GrowNYC          19800\n5       Seattle, WA  @seattleFarmMkts          17100\n6  Santa Monica, CA            @smfms           8291\n7    Des Moines, IA  @DTFarmersMarket           7391\n8        Austin, TX         @SFClocal           5498\n9       Madison, WI         @DaneCoFM           2805"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_markets = pd.read_excel('./list_of_farmers_markets.xlsx')\n",
    "list_markets = list_markets.sort_values(by=['Num_Followers'], ascending=False)\n",
    "list_markets = list_markets.reset_index(drop=True)\n",
    "list_markets.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Defining the number of people to pick from each city; for now I am choosing\n",
    "# # this number as being proportional to the total number of follower\n",
    "# list_markets['Num_Followers_ToPick'] = (list_markets['Num_Followers']/\n",
    "#                                         np.sum(list_markets['Num_Followers'])*2000).astype(np.int)\n",
    "# list_markets.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Authenticating the Twitter API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer_key = config.consumer_key\n",
    "consumer_secret = config.consumer_secret\n",
    "auth = tweepy.AppAuthHandler(consumer_key, consumer_secret)\n",
    "\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "1994"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Downloading the user handles i.e. `screen_name` of users\n",
    "\n",
    "First, we begin with some helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "We iterature through the list of screen_names and we download all other follower_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In future runs, if you don't have to download this data again,\n",
    "# just load the original pickle file\n",
    "# followers_dict = {}\n",
    "# for market in tqdm(list_markets['Twitter_handle']):\n",
    "#     try:\n",
    "#         followers = tweepy.Cursor(api.followers,\n",
    "#                                     screen_name=market,\n",
    "#                                     lang='en',\n",
    "#                                     include_entities=True,\n",
    "#                                     count=2000).items(2000)\n",
    "#         followers_list = list(followers)\n",
    "#         followers_json = list(map(lambda f: f._json, followers_list))\n",
    "#         followers_dict[market] = followers_dict.get(market, []) + followers_json\n",
    "#     except:\n",
    "#         with open('./followers_dict.data', 'wb') as filehandle:\n",
    "#             pickle.dump(followers_dict, filehandle)\n",
    "\n",
    "# with open('./followers_dict.data', 'wb') as filehandle:\n",
    "#     pickle.dump(followers_dict, filehandle)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I now have a dictionary of the form\n",
    "\n",
    "```\n",
    "{\n",
    "    market1: [{user1_json}, {user2_json}, ..., {user2000_json}],\n",
    "    market2: [{user1_json}, {user2_json}, ..., {user2000_json}],\n",
    "    .\n",
    "    .\n",
    "    market10: [{user1_json}, {user2_json}, ..., {user2000_json}],    \n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Trimming `follower_dict`\n",
    "\n",
    "Now that I have the `follower_dict`, I have a lot of users along with all their metadata. To really distinguish between users who provide signal and users who provide noise, I choose two parameters: users who have more than 500 followers themselves, and users who have tweeted out more than 500 times (relax this second condition if 500 is too high - I don't have a sense for how high this should be). I have 10 x 1000 total followers so hopefully I will find enough users with over 500 followers and over 500 tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Defining the function that selects followers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selected_follower(follower):\n",
    "    \"\"\"\n",
    "    input: Accepts a follower json and then checkes to see if they have over 500 followers and have tweeted over 500 times.\n",
    "    returns: Boolean if criteria are met \n",
    "    \"\"\"\n",
    "    followers_bool = False\n",
    "    tweets_bool = False\n",
    "    if follower['followers_count'] >= 150:\n",
    "        followers_bool = True\n",
    "    if follower['statuses_count'] >= 500:\n",
    "        tweets_bool = True\n",
    "    return followers_bool and tweets_bool"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###  Making the `master_dict` only with the selected followers\n",
    "\n",
    "Use only if downloading new data. Otherwise, go ahead and use the file that has been exported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "100%|██████████| 10/10 [00:00<00:00, 435.22it/s]\n6665\n"
    }
   ],
   "source": [
    "with open('./data/followers_dict.data', 'rb') as filehandle:\n",
    "    followers_dict = pickle.load(filehandle)\n",
    "\n",
    "counter = 0\n",
    "follower_dict_trimmed = collections.defaultdict(lambda: [])\n",
    "for market in tqdm(followers_dict):\n",
    "    followers = followers_dict[market]\n",
    "    for follower in followers:\n",
    "        if selected_follower(follower):\n",
    "            counter += 1\n",
    "            follower_dict_trimmed[market] = follower_dict_trimmed[market] + [follower]\n",
    "print(counter)\n",
    "# With these criterion, I get 6665 unique followers. I next download 500 tweets from each one of those 6665 followers. Perhaps this will give me enough diversity and a large enough corpus of words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/followers_dict_trimmed.data', 'wb') as filehandle:\n",
    "    pickle.dump(dict(follower_dict_trimmed), filehandle)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "collections.defaultdict"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Downloading 500 tweets from each of the selected followers\n",
    "\n",
    "I am going to retain the market split because I want documents grouped by market. i.e. I am looking for a dictionary of the following structure:\n",
    "\n",
    "```\n",
    "{\n",
    "    market1: {\n",
    "                user1: [{tweet1_json}, {tweet2_json}, ..., {tweetn_json}],\n",
    "                user2: [{tweet1_json}, {tweet2_json}, ..., {tweetn_json}],\n",
    "                .\n",
    "                .\n",
    "                usern: [{tweet1_json}, {tweet2_json}, ..., {tweetn_json}]\n",
    "            }\n",
    "    .\n",
    "    .\n",
    "    market10: {\n",
    "                user1: [{tweet1_json}, {tweet2_json}, ..., {tweetn_json}],\n",
    "                user2: [{tweet1_json}, {tweet2_json}, ..., {tweetn_json}],\n",
    "                .\n",
    "                .\n",
    "                usern: [{tweet1_json}, {tweet2_json}, ..., {tweetn_json}]\n",
    "              }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "100%|██████████| 703/703 [30:17<00:00,  1.99s/it]\n100%|██████████| 434/434 [19:21<00:00,  1.83s/it]\n100%|██████████| 723/723 [37:21<00:00,  2.89s/it]\n"
    }
   ],
   "source": [
    "with open('./data/followers_dict_trimmed.data', 'rb') as filehandle:\n",
    "    follower_dict_trimmed = pickle.load(filehandle)\n",
    "\n",
    "all_tweets = {}\n",
    "markets = list(follower_dict_trimmed.keys())\n",
    "# Note - I have already downloaded for first three markets. Try and\n",
    "# pick up here i.e. go [3:x] in the future after uploading the current\n",
    "# dictionary. \n",
    "\n",
    "# Load all_tweets_dict.data if needed and just add the new market keys.\n",
    "for market in markets[:3]:\n",
    "    all_tweets[market] = {}\n",
    "    followers = follower_dict_trimmed[market]\n",
    "    for follower in tqdm(followers):\n",
    "        try:\n",
    "            screen_name = follower['screen_name']\n",
    "            tweets = tweepy.Cursor(api.user_timeline,\n",
    "                                    screen_name=screen_name,\n",
    "                                    tweet_mode='extended',\n",
    "                                    count=500).items(500)\n",
    "            for tweet in tweets:\n",
    "                all_tweets[market][screen_name] = all_tweets[market].get(screen_name, []) + [tweet._json]\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "with open('./data/all_tweets_dict.data', 'wb') as filehandle:\n",
    "    pickle.dump(all_tweets, filehandle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Note on the structure of the output dictionary: It has the form\n",
    "\n",
    "```\n",
    "{\n",
    "    market1: {\n",
    "                screen_name_1: [{tweet1, ..., tweetn}],\n",
    "                .\n",
    "                .\n",
    "                .\n",
    "                screen_name_m: [{tweet1, ..., tweetn}]\n",
    "            }\n",
    "    .\n",
    "    .\n",
    "    .\n",
    "    market10: {\n",
    "                screen_name_1: [{tweet1, ..., tweetn}],\n",
    "                .\n",
    "                .\n",
    "                .\n",
    "                screen_name_2028: [{tweet1, ..., tweetn}]\n",
    "             }\n",
    "}\n",
    "```"
   ]
  }
 ]
}