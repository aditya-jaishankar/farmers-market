{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Dirichlet Allocation\n",
    "\n",
    "This is the main chunk of the code.\n",
    "\n",
    "The eventual goal is to treat the hashtag list for each user as being document1, and the cleaned full-text words as being document 2. So each user has two documents. Now I do topic modeling across each document for each user and for each user find a list of topics, and then the words that lie within each topic. Therefore, I have now for each user a dictionary with keys as topics and values as the words associated with each topic. What I am then hoping to do is some sort of visualization to extract the most relevant topics that exhibit the words that I am interested in. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports\n",
    "import json\n",
    "import glob\n",
    "import pickle\n",
    "import collections\n",
    "import random\n",
    "from tqdm import tqdm as tqdm\n",
    "import config\n",
    "import time\n",
    "import os\n",
    "dirpath = os.path.dirname(os.path.realpath('__file__'))\n",
    "from pprint import pprint\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\n",
    "                    filename='./lda_logging.log',\n",
    "                    level=logging.DEBUG)\n",
    "\n",
    "# NLP imports\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['https', 'http'])\n",
    "import re\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.models import CoherenceModel\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "\n",
    "# To check later if a words is in english or not\n",
    "with open('./words_dictionary.json') as filehandle:\n",
    "    words_dictionary = json.load(filehandle)\n",
    "english_words = words_dictionary.keys()\n",
    "\n",
    "# Visualization imports\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "pyLDAvis.enable_notebook()\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "# Other imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tweepy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the cleaned and simplified tweet dictionary\n",
    "\n",
    "### Note on the format of input and output dictionaries\n",
    "\n",
    "Here, we first load in the dictionaries that were dumped in as pickle files and then do a series of text processing and cleaning tasks. I initially start with a dicitonary of the form:\n",
    "\n",
    "```\n",
    "{\n",
    "    market_1: {\n",
    "                screen_name_1: [{tweet1, ..., tweetn}],\n",
    "                .\n",
    "                .\n",
    "                .\n",
    "                screen_name_m: [{tweet1, ..., tweetn}]\n",
    "             }\n",
    "    .\n",
    "    .\n",
    "    .\n",
    "    market_k: {\n",
    "                screen_name_1: [{tweet1, ..., tweetn}],\n",
    "                .\n",
    "                .\n",
    "                .\n",
    "                screen_name_m: [{tweet1, ..., tweetn}]\n",
    "             }\n",
    "}\n",
    "```\n",
    "\n",
    "This section of the code will then process will result in a dictionary of the form\n",
    "\n",
    "```\n",
    "{\n",
    "    market_1: {\n",
    "                screen_name_1: \n",
    "                    {\n",
    "                        hashtags: [list of hashtags from each tweet], \n",
    "                        fulltext: [list of all cleaned/depunkt words across all tweets]\n",
    "                    },\n",
    "                .\n",
    "                .\n",
    "                screen_name_m: \n",
    "                    {\n",
    "                        hashtags: [list of hashtags from each tweet], \n",
    "                        fulltext: [list of all cleaned/depunkt words across all tweets]\n",
    "                    }\n",
    "              }\n",
    "    .\n",
    "    .\n",
    "    .\n",
    "    market_k: {\n",
    "                screen_name_1: \n",
    "                    {\n",
    "                        hashtags: [list of hashtags from each tweet], \n",
    "                        fulltext: [list of all cleaned/depunkt words across all tweets]\n",
    "                    },\n",
    "                .\n",
    "                .\n",
    "                screen_name_m: \n",
    "                    {\n",
    "                        hashtags: [list of hashtags from each tweet], \n",
    "                        fulltext: [list of all cleaned/depunkt words across all tweets]\n",
    "                    }\n",
    "              }\n",
    "}\n",
    "```\n",
    "\n",
    "Then I can turn this into a pandas dataframe and do some pretty nice data manipulation.\n",
    "\n",
    "We will call this dictionary the `master_dict`.\n",
    "\n",
    "To do this, we first define some helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Defining some utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "C:\\Users\\adity\\AppData\\Local\\Continuum\\anaconda3\\envs\\insightenv\\lib\\site-packages\\smart_open\\smart_open_lib.py:402: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\nC:\\Users\\adity\\AppData\\Local\\Continuum\\anaconda3\\envs\\insightenv\\lib\\site-packages\\smart_open\\smart_open_lib.py:402: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
    }
   ],
   "source": [
    "def get_user(tweet):\n",
    "    \"\"\"\n",
    "    input: tweet dictionary\n",
    "    returns: return the username\n",
    "    \"\"\"\n",
    "    return tweet['user']['screen_name']\n",
    "\n",
    "\n",
    "def get_hashtag_list(tweet):\n",
    "    \"\"\"\n",
    "    input: tweet dictionary\n",
    "    returns: list of all hashtags in both the direct tweet and the\n",
    "    retweet \n",
    "    \"\"\"\n",
    "\n",
    "    l = []\n",
    "    for d in tweet['entities']['hashtags']:\n",
    "        l += [d['text']]\n",
    "\n",
    "    if 'retweeted_status' in tweet.keys():\n",
    "        for d in tweet['retweeted_status']['entities']['hashtags']:\n",
    "            l += [d['text']]\n",
    "    return l\n",
    "\n",
    "\n",
    "def tokenizer_cleaner_nostop_lemmatizer(text):\n",
    "    \"\"\"\n",
    "    This function tokenizes the text of a tweet, cleans it off punctuation,\n",
    "    removes stop words, and lemmatizes the words (i.e. finds word roots to remove noise)\n",
    "    I am largely using the gensim and spacy packages \n",
    "\n",
    "    Input: Some text\n",
    "    Output: List of tokenized, cleaned, lemmatized words\n",
    "    \"\"\"\n",
    "\n",
    "    tokenized_depunkt = gensim.utils.simple_preprocess(text, min_len=4, deacc=True)\n",
    "    tokenized_depunkt_nostop = ([word for word in tokenized_depunkt \n",
    "                                 if (word not in stop_words and word in english_words)])\n",
    "    \n",
    "    # Lemmatizer while also only allowing certain parts of speech.\n",
    "    # See here: https://spacy.io/api/annotation\n",
    "    allowed_pos = ['ADJ', 'ADV', 'NOUN', 'PROPN','VERB'] # try removing propn\n",
    "    # allowed_pos = ['ADJ', 'ADV', 'NOUN','VERB'] # try removing propn and seeing\n",
    "    doc = nlp(' '.join(tokenized_depunkt_nostop))\n",
    "    words_final = [token.lemma_ for token in doc if token.pos_ in allowed_pos]\n",
    "    return words_final\n",
    "\n",
    "    \n",
    "def get_tweet_words_list(tweet):\n",
    "    \"\"\"\n",
    "    This function takes in a tweet and checks if there is a retweet associated with it\n",
    "    input: tweet\n",
    "    output: list of tokenized words without punctuation\n",
    "    \"\"\"\n",
    "\n",
    "    text = tweet['full_text']\n",
    "    clean_words = tokenizer_cleaner_nostop_lemmatizer(text)\n",
    "    \n",
    "    if 'retweeted_status' in tweet.keys():\n",
    "        retweet_text = tweet['retweeted_status']['full_text']\n",
    "        retweet_clean_words = tokenizer_cleaner_nostop_lemmatizer(retweet_text)\n",
    "        clean_words += retweet_clean_words\n",
    "    return clean_words\n",
    "\n",
    "# # load the classifer model and the commercial filter LDA model\n",
    "# with open('./models/commercial_follower_classifier.clf', 'rb') as filehandle:\n",
    "#     clf = pickle.load(filehandle)\n",
    "\n",
    "lda_model_path = './ldamodels/market_followers/model.model'\n",
    "lda_model_market_followers = gensim.models.ldamodel.LdaModel.load(\n",
    "                                                        lda_model_path)\n",
    "\n",
    "with open('./ldamodels/market_followers/corpus.corpus', 'rb') as filehandle:\n",
    "    market_followers_corpus = pickle.load(filehandle)\n",
    "\n",
    "def get_augmented_feature_vectors(feature_vectors):\n",
    "    \"\"\"\n",
    "    Takes in the feature vector list of list and augments it. gensim does not\n",
    "    actually put a 0 for topics that have 0 probability so I need to manually\n",
    "    add it in to build my feature vector. \n",
    "    input: accepts the feature vectors output by gensim. It is a list of \n",
    "    tuples - one list entry per document and tuple are (topic, probability)\n",
    "    pairs.\n",
    "    returns: Augmented feature vectors as list of list. Each list entry \n",
    "    corresponds to one document, with the i-th element in the inner list\n",
    "    corresponding to the probability that the document was generated with \n",
    "    topic i.\n",
    "    \"\"\"\n",
    "    augmented = []\n",
    "    for i, vector in enumerate(feature_vectors): # each vector is a list of tuples\n",
    "        topics = [tup[0] for tup in vector]\n",
    "        for t in range(10):\n",
    "            if t not in topics:\n",
    "                feature_vectors[i].append((t, 0))\n",
    "        new_feature_vector = sorted(feature_vectors[i], key=lambda tup: tup[0])\n",
    "        augmented.append([tup[1] for tup in new_feature_vector])\n",
    "    return augmented\n",
    "\n",
    "def feature_vector_commercial_model(doc):\n",
    "    \"\"\"\n",
    "    This function accepts a document and then makes an inference with the\n",
    "    lda model trained on the commercial/random user dataset. It then\n",
    "    returns the feature vector which consists of the probabilities that document\n",
    "    was generated by topic i. Note that this feature vector is unaugmented, and we\n",
    "    will call the augmentation function to add zeros appropriately.\n",
    "    \n",
    "    input: document consisting of the full text of all tweets of a particular user\n",
    "    returns: probability feature vector. \n",
    "    \"\"\"\n",
    "    id2word = corpora.Dictionary([doc]) # To satisfy that id2word needs a list of lists\n",
    "    corpus = id2word.doc2bow(doc)\n",
    "    topics = lda_model_market_followers.get_document_topics(corpus)\n",
    "    return get_augmented_feature_vectors([topics]) # for consistency in dimensions\n",
    "\n",
    "def commercial_Q(feature_vector):\n",
    "    \"\"\"\n",
    "    This function uses the pre-trained binary classifer to make a prediction on whether\n",
    "    given the full text document of a given user, whether it is commercial or not.\n",
    "\n",
    "    input: feature vector\n",
    "    returns: scalar prediction on commercial label (1 if yes)\n",
    "    \"\"\"\n",
    "\n",
    "    return clf.predict(feature_vector)[0]\n",
    "\n",
    "# The workflow above upto and including commercial_Q all passed some basic unit tests\n",
    "# showing that it functions correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating the `master_dict`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Location</th>\n      <th>Twitter Tag</th>\n      <th>Num_Followers</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Philadelphia, PA</td>\n      <td>@thefoodtrust</td>\n      <td>37000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>New York, NY</td>\n      <td>@unsqgreenmarket</td>\n      <td>25500</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Chicago, IL</td>\n      <td>@greencitymarket</td>\n      <td>24600</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Seattle, WA</td>\n      <td>@seattleFarmMkts</td>\n      <td>17100</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Santa Monica, CA</td>\n      <td>@smfms</td>\n      <td>8291</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Des Moines, IA</td>\n      <td>@DTFarmersMarket</td>\n      <td>7391</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>Austin, TX</td>\n      <td>@SFClocal</td>\n      <td>5498</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "           Location       Twitter Tag  Num_Followers\n0  Philadelphia, PA     @thefoodtrust          37000\n1      New York, NY  @unsqgreenmarket          25500\n2       Chicago, IL  @greencitymarket          24600\n3       Seattle, WA  @seattleFarmMkts          17100\n4  Santa Monica, CA            @smfms           8291\n5    Des Moines, IA  @DTFarmersMarket           7391\n6        Austin, TX         @SFClocal           5498"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First we load the list of market names to be able to use the data files\n",
    "\n",
    "list_markets = pd.read_excel('./list_of_farmers_markets.xlsx')\n",
    "list_markets = list_markets.sort_values(by=['Num_Followers'], ascending=False)\n",
    "list_markets = list_markets.reset_index(drop=True)\n",
    "\n",
    "# Based on some data inspection, we remove portland, remove GrowNYC and Madison. \n",
    "list_markets = list_markets.drop([1, 4, 9]).reset_index(drop=True)\n",
    "list_markets.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # with open('./data/all_tweets_dict.data', 'rb') as filehandle:\n",
    "# #     all_tweets_data = pickle.load(filehandle)\n",
    "\n",
    "# master_dict = {}\n",
    "\n",
    "# for market in list_markets['Twitter Tag']:\n",
    "#     filename = './data/all_tweets_dict_' + market + '.data'\n",
    "\n",
    "#     with open(filename, 'rb') as filehandle:\n",
    "#         all_tweets_data = pickle.load(filehandle)\n",
    "\n",
    "#     # followers = all_tweets_data[market]\n",
    "#     market_name = market[1:]\n",
    "#     master_dict[market_name] = {}\n",
    "\n",
    "#     for follower in tqdm(all_tweets_data):\n",
    "#         tweets = all_tweets_data[follower] # list of tweet_.json\n",
    "#         master_dict[market_name][follower] = {}\n",
    "#         master_dict[market_name][follower]['hashtags'] = []\n",
    "#         master_dict[market_name][follower]['fulltext'] = []\n",
    "#         for tweet in tweets:\n",
    "#             hashtags = get_hashtag_list(tweet)\n",
    "#             words = get_tweet_words_list(tweet)\n",
    "            \n",
    "#             master_dict[market_name][follower]['hashtags'].extend(hashtags)\n",
    "#             master_dict[market_name][follower]['fulltext'].extend(words)\n",
    "\n",
    "# with open('./data/master_dict.data', 'wb') as filehandle:\n",
    "#     pickle.dump(master_dict, filehandle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the Latent Dirichlet Allocation\n",
    "\n",
    "Now we apply the LDA algorithm to identify themes in the documents/topics. In my case, a single document corresponds to the set of all words of a single user's tweets. Note that the list of words that comprose a document have already been cleaned, tokenized and lemmatized. \n",
    "\n",
    "One other thought is to have one MASSIVE document containing all tweets of all users, and then finding the topics there. In the comparison step, I could use these top topics and then compare this to all tweets of individual users and then returning top-k users based on similarity. See [this](https://stats.stackexchange.com/questions/269031/how-to-find-similar-documents-after-a-latent-dirichlet-allocation-model-is-bui) stack-exchange post for ideas.\n",
    "\n",
    "We first write some helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/master_dict.data', 'rb') as filehandle:\n",
    "    master_dict = pickle.load(filehandle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_docs(d, market):\n",
    "    \"\"\"\n",
    "    Accepts a market and then returns the documents for the market. A document\n",
    "    is a list of of word lists for each user in the market city i.e. it is a list of lists.\n",
    "    Each outer list is a follower and the innner list is the cleaner, tokenized, depunkt, \n",
    "    lematized set of words for that follower.\n",
    "    \"\"\"\n",
    "    docs = []\n",
    "    for user in d[market]:\n",
    "        text_list = d[market][user]['fulltext']\n",
    "        docs.append(text_list)\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "markets = list(master_dict.keys())\n",
    "\n",
    "def get_id2word_corpus(market_index, split):\n",
    "    \"\"\"\n",
    "    accept a market index and a split so that the corpus can be decided according to that.\n",
    "    \"\"\"\n",
    "    docs = get_docs(master_dict, markets[market_index])\n",
    "    # np.random.shuffle(docs)\n",
    "    num_docs = len(docs)\n",
    "\n",
    "    if split == 1:\n",
    "        docs = docs[:num_docs//2]\n",
    "    else:\n",
    "        docs = docs[num_docs//2:]\n",
    "\n",
    "    # use the commercial_Q filter to see if a document needs to be included. \n",
    "    # For now, I am using a for loop to keep track of the count of how many documents are\n",
    "    # rejected, but can probably change to list comprehension\n",
    "\n",
    "    docs_filtered = []\n",
    "    for doc in docs:\n",
    "        docs_filtered.append(doc)\n",
    "        # f = feature_vector_commercial_model(doc)\n",
    "        # if commercial_Q(f) == 1 and random.choice([0, 1, 2]) == 1: \n",
    "            # 1 means it is a commercial doc. Only reject half the docs\n",
    "            # docs_filtered = docs_filtered[:-1]\n",
    "    print('Total documents:', len(docs), ', Documents accepted:', len(docs_filtered))\n",
    "    id2word = corpora.Dictionary(docs_filtered)\n",
    "\n",
    "    # Idea: Keep only those tokens that appear in at least 10% of the documents\n",
    "    id2word.filter_extremes(no_below=int(0.1*len(docs_filtered)))\n",
    "    corpus = [id2word.doc2bow(doc) for doc in docs_filtered]\n",
    "    return (id2word, corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_lda(corpus, id2word, k=7, alpha='auto'):\n",
    "    \"\"\"\n",
    "    Performs the LDA and returns the computer model.\n",
    "    Input: Corpus, dictionary and hyperparameters to optimize\n",
    "    Output: the fitted/computed LDA model\n",
    "    \"\"\"\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus, \n",
    "                                                    id2word=id2word,\n",
    "                                                    num_topics=k,\n",
    "                                                    random_state=100,\n",
    "                                                    update_every=1,\n",
    "                                                    eval_every=1,\n",
    "                                                    chunksize=5,\n",
    "                                                    passes=20,\n",
    "                                                    alpha=.01,\n",
    "                                                    iterations=1000,\n",
    "                                                    per_word_topics=True)\n",
    "    return lda_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Total documents:392, Documents accepted:392\n[(0,\n'0.032*\"climate\" + 0.013*\"risk\" + 0.011*\"legal\" + 0.008*\"reality\" + '\n'0.008*\"artist\" + 0.008*\"planet\" + 0.007*\"intelligence\" + 0.007*\"rule\" + '\n'0.006*\"kill\" + 0.006*\"government\" + 0.006*\"model\" + 0.006*\"force\" + '\n'0.006*\"shoot\" + 0.006*\"content\" + 0.006*\"white\" + 0.006*\"police\" + '\n'0.006*\"brain\" + 0.005*\"screen\" + 0.005*\"seem\" + 0.005*\"demand\"'),\n(1,\n'0.031*\"sustainable\" + 0.031*\"farmer\" + 0.028*\"farm\" + 0.023*\"nutrition\" + '\n'0.020*\"protein\" + 0.015*\"insect\" + 0.011*\"waste\" + 0.011*\"animal\" + '\n'0.010*\"style\" + 0.009*\"bird\" + 0.009*\"organic\" + 0.008*\"nature\" + '\n'0.008*\"environment\" + 0.008*\"garden\" + 0.008*\"natural\" + 0.008*\"diet\" + '\n'0.008*\"feed\" + 0.008*\"spring\" + 0.008*\"research\" + 0.008*\"farming\"'),\n(2,\n'0.017*\"meditation\" + 0.016*\"cancer\" + 0.015*\"wellness\" + 0.011*\"street\" + '\n'0.010*\"quite\" + 0.009*\"college\" + 0.009*\"email\" + 0.009*\"test\" + '\n'0.008*\"color\" + 0.008*\"mention\" + 0.008*\"discover\" + 0.008*\"body\" + '\n'0.008*\"baby\" + 0.007*\"extremely\" + 0.007*\"lifestyle\" + 0.007*\"beat\" + '\n'0.007*\"interview\" + 0.007*\"episode\" + 0.007*\"lady\" + 0.007*\"target\"'),\n(3,\n'0.034*\"register\" + 0.027*\"education\" + 0.025*\"teacher\" + '\n'0.014*\"professional\" + 0.014*\"district\" + 0.013*\"explore\" + '\n'0.012*\"innovation\" + 0.011*\"yoga\" + 0.011*\"science\" + 0.011*\"center\" + '\n'0.010*\"proposal\" + 0.010*\"volunteer\" + 0.010*\"educator\" + '\n'0.009*\"registration\" + 0.009*\"discussion\" + 0.008*\"learning\" + '\n'0.008*\"exercise\" + 0.008*\"career\" + 0.008*\"ensure\" + 0.008*\"chat\"'),\n(4,\n'0.043*\"restaurant\" + 0.035*\"recipe\" + 0.029*\"meal\" + 0.025*\"beer\" + '\n'0.023*\"cocktail\" + 0.023*\"wine\" + 0.022*\"taste\" + 0.021*\"chef\" + '\n'0.021*\"delicious\" + 0.019*\"breakfast\" + 0.018*\"cook\" + 0.015*\"salad\" + '\n'0.014*\"flavor\" + 0.013*\"chicken\" + 0.013*\"pizza\" + 0.013*\"chocolate\" + '\n'0.012*\"menu\" + 0.010*\"burger\" + 0.010*\"tasty\" + 0.010*\"snack\"'),\n(5,\n'0.061*\"conference\" + 0.025*\"skill\" + 0.024*\"session\" + 0.021*\"energy\" + '\n'0.021*\"schedule\" + 0.021*\"building\" + 0.020*\"youth\" + '\n'0.018*\"congratulation\" + 0.017*\"tech\" + 0.017*\"technology\" + '\n'0.014*\"development\" + 0.013*\"partnership\" + 0.013*\"connect\" + '\n'0.013*\"involve\" + 0.013*\"engage\" + 0.012*\"interested\" + 0.012*\"director\" + '\n'0.012*\"lesson\" + 0.012*\"congrat\" + 0.012*\"staff\"'),\n(6,\n'0.017*\"homeless\" + 0.016*\"solar\" + 0.015*\"agriculture\" + 0.010*\"policy\" + '\n'0.009*\"entrepreneur\" + 0.009*\"classroom\" + 0.008*\"global\" + 0.008*\"summit\" '\n'+ 0.007*\"workshop\" + 0.007*\"hunger\" + 0.006*\"homelessness\" + 0.006*\"native\" '\n'+ 0.006*\"twitter\" + 0.006*\"address\" + 0.006*\"control\" + 0.006*\"rate\" + '\n'0.005*\"review\" + 0.005*\"regional\" + 0.005*\"fashion\" + 0.005*\"weight\"')]\n"
    }
   ],
   "source": [
    "# for market_index in tqdm(range(7)):\n",
    "market_index = 6\n",
    "split_index = 2\n",
    "id2word, corpus = get_id2word_corpus(market_index, split_index)\n",
    "lda_model = compute_lda(corpus, id2word)\n",
    "# save the model\n",
    "# filename_model = './ldamodels/market' + str(market_index) + '/model.model'\n",
    "# lda_model.save(filename_model)\n",
    "# # save the corpus\n",
    "# filename_corpus = './ldamodels/market' + str(market_index) + '/corpus.corpus'\n",
    "# with open(filename_corpus, 'wb') as filehandle:\n",
    "#     pickle.dump(corpus, filehandle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "pprint(lda_model.print_topics(num_topics=-1, num_words=20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some comments on the hyperparameter tuning:\n",
    "\n",
    "1. Doing a chunksize of 1 is pretty slow and time consuming (although it might be worthwhile to time this more accurately). I think either choosing `chunksize=5` or `chunksize=10` works well.\n",
    "2. `passes` is a parameter similar to number of epochs. \n",
    "3. `alpha='auto'` seems to work pretty well. \n",
    "4. Keep `random_state=100` in case you want to repeat results. \n",
    "5. Keep `update_every` small, ideally equal to 1. \n",
    "6. Use a for loop to calculate the optimal number of topics. This just has to be done. Might be better to do this in the background on the python terminal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating the optimal number of topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimal_topics():\n",
    "    coherence_scores = []\n",
    "    for k in tqdm(range(1, 10)):\n",
    "        lda_model = compute_lda(corpus, id2word, k=k)\n",
    "        coherence_model_lda = CoherenceModel(model=lda_model,\n",
    "                                        texts=docs_filtered,\n",
    "                                        dictionary=id2word,\n",
    "                                        coherence='c_v')\n",
    "        coherence_lda = coherence_model_lda.get_coherence()\n",
    "        print((k, coherence_lda))\n",
    "        coherence_scores.append(coherence_lda)\n",
    "    return coherence_scores\n",
    "    \n",
    "coherence_scores = optimal_topics()\n",
    "plt.plot(range(1,10), coherence_scores)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3.6.8 64-bit ('insightenv': conda)",
   "language": "python",
   "name": "python36864bitinsightenvconda4d948c2035f94dc3b09753f05f123a75"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8-final"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}