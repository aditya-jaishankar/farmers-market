{"cells":[{"cell_type":"markdown","source":[" # Identifying commercial markets to filter them out from followers\n","\n"," The goal of this file is to be able to identify which of the followers that I\n","  selected are commercial followers or otherwise small businesses. This\n"," corrupts my input user base with non-people, so this is an attempt to remove\n"," these followers."],"metadata":{}},{"cell_type":"markdown","source":[" ## Imports"],"metadata":{}},{"source":["import json\n","import glob\n","import pickle\n","import collections\n","import random\n","from tqdm import tqdm as tqdm\n","import time\n","import config\n","\n","\n","import os\n","dirpath = os.path.dirname(os.path.realpath('__file__'))\n","\n","import tweepy\n","import config\n","\n","# NLP imports\n","import nltk\n","from nltk.corpus import stopwords\n","stop_words = stopwords.words('english')\n","stop_words.extend(['https', 'http'])\n","import re\n","import gensim\n","import gensim.corpora as corpora\n","from gensim.models import CoherenceModel\n","import spacy\n","nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n","\n","# To check later if a words is in english or not. Note that to include some\n","# additional words as stop words, I just removed them from this dictionary\n","with open('./words_dictionary.json') as filehandle:\n","    words_dictionary = json.load(filehandle)\n","english_words = words_dictionary.keys()\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","\n","# geo imports\n","import shapefile\n","from shapely.geometry import Point # Point class\n","from shapely.geometry import shape # shape() is a function to convert geo objects through the interface\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":["# All functions"],"metadata":{}},{"source":["def is_urban(pt, shapes):\n","    \"\"\"\n","    Takes in a point and then checks to see if it lies within an urban boundary\n","    returns: Boolean\n","    \"\"\"\n","    for boundary in shapes:\n","        if Point(pt).within(shape(boundary)): \n","            return True\n","    return False\n","\n","def handle_applier(handle):\n","    \"\"\"\n","    Some twitter handles have the '@' symbol which shouldn't be \n","    passed to the twitter API so we will remove them\n","    \"\"\"\n","    if len(handle) == 0:\n","        return handle\n","    if handle[0] == '@':\n","        return handle[1:]\n","    return handle\n","\n","def id_generator():\n","    \"\"\"\n","    Generates user_id integers to supply to twitter while generating random\n","    users. Note that the docs say that these numbers are 64 bit unsigned\n","    integers (https://developer.twitter.com/en/docs/basics/twitter-ids)\n","    input: none\n","    output: a random 64-bit unsigned integer\n","    \"\"\"\n","    # Pick bit length at random\n","    bit_length = np.random.randint(low=10, high=50)\n","    \n","    # Pick the bits at random\n","    digits = np.random.randint(low=0, high=2, size=bit_length)\n","    binary_number = ''.join(list(map(str, digits)))\n","    return int(binary_number, 2)\n","\n","def get_user(tweet):\n","    \"\"\"\n","    input: tweet dictionary\n","    returns: return the username\n","    \"\"\"\n","    return tweet['user']['screen_name']\n","\n","\n","def get_hashtag_list(tweet):\n","    \"\"\"\n","    input: tweet dictionary\n","    returns: list of all hashtags in both the direct tweet and the\n","    retweet \n","    \"\"\"\n","\n","    l = []\n","    for d in tweet['entities']['hashtags']:\n","        l += [d['text']]\n","\n","    if 'retweeted_status' in tweet.keys():\n","        for d in tweet['retweeted_status']['entities']['hashtags']:\n","            l += [d['text']]\n","    return l\n","\n","\n","def tokenizer_cleaner_nostop_lemmatizer(text):\n","    \"\"\"\n","    This function tokenizes the text of a tweet, cleans it off punctuation,\n","    removes stop words, and lemmatizes the words (i.e. finds word roots to remove noise)\n","    I am largely using the gensim and spacy packages \n","\n","    Input: Some text\n","    Output: List of tokenized, cleaned, lemmatized words\n","    \"\"\"\n","\n","    tokenized_depunkt = gensim.utils.simple_preprocess(text, min_len=4, deacc=True)\n","    tokenized_depunkt_nostop = ([word for word in tokenized_depunkt \n","                                 if (word not in stop_words and word in english_words)])\n","    \n","    # Lemmatizer while also only allowing certain parts of speech.\n","    # See here: https://spacy.io/api/annotation\n","    allowed_pos = ['ADJ', 'ADV', 'NOUN', 'PROPN','VERB']\n","    doc = nlp(' '.join(tokenized_depunkt_nostop))\n","    words_final = [token.lemma_ for token in doc if token.pos_ in allowed_pos]\n","    return words_final\n","\n","    \n","def get_tweet_words_list(tweet):\n","    \"\"\"\n","    This function takes in a tweet and checks if there is a retweet associated \n","    with it. It then returns a list of tokenized words without punctuation.\n","    input: tweet\n","    output: list of tokenized words without punctuation\n","    \"\"\"\n","\n","    text = tweet['full_text']\n","    clean_words = tokenizer_cleaner_nostop_lemmatizer(text)\n","    \n","    if 'retweeted_status' in tweet.keys():\n","        retweet_text = tweet['retweeted_status']['full_text']\n","        retweet_clean_words = tokenizer_cleaner_nostop_lemmatizer(retweet_text)\n","        clean_words += retweet_clean_words\n","    return clean_words\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[" ## Loading the data and adding an `is_urban` column\n","\n"," We get the data of urban regions from US Census data, and we write a simple\n"," for loop to see if a particular city lies in an urban region. I only want to\n"," select users from urban areas."],"metadata":{}},{"source":["load_columns = ['Twitter', 'city', 'State', 'x', 'y', 'zip' ]\n","markets = pd.read_csv('./farmers_market_twitter_all.csv', usecols=load_columns)\n","markets = markets.dropna(axis=0)\n","markets['coords'] = markets.apply(lambda row: (row['x'], row['y']), axis=1)\n","\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["# shp = shapefile.Reader('./data/tl_2019_us_uac10/tl_2019_us_uac10.shp')\n","# shapes = shp.shapes() # get all the polygons\n","\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["# tqdm.pandas()\n","# markets['is_urban'] = markets.progress_apply(lambda row: is_urban(row['coords'],\n","#                                                             shapes), axis=1)\n","# markets = markets[markets['is_urban']] # select only the urban_areas\n","# print('Number of eligible markets:', markets.shape[0])\n","# # export to csv so I don't have to do this again\n","# markets.to_csv('./data/markets_is_urban.csv', index=False)\n","\n","markets = pd.read_csv('./data/markets_is_urban.csv')\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[" ### I still need to further process the twitter handle because they are all\n"," over the place"],"metadata":{}},{"source":["\n","# Remove all the '#NAME?' entries. Decreases the number of markets to 789.\n","markets = markets[markets['Twitter'] != '#NAME?']\n","# Only extract the bits after the last slash\n","markets['Twitter'] = markets['Twitter'].apply(lambda r: r.rsplit('/', 1)[-1])\n","markets['Twitter'] = markets.apply(lambda r: handle_applier(r['Twitter']), \n","                                                            axis=1)\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[" ## Download upto a 1000 tweets for farmers markets that lie in urban areas."],"metadata":{}},{"source":["consumer_key = config.consumer_key\n","consumer_secret = config.consumer_secret\n","auth = tweepy.AppAuthHandler(consumer_key, consumer_secret)\n","\n","api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[" ## Downloading 1000 tweets for each eligible market"],"metadata":{}},{"source":["# In future runs, if you don't have to download this data again,\n","# just load the original pickle file\n","# market_tweets_dict = {}\n","# for market in tqdm(markets['Twitter']):\n","#     try:\n","#         tweets = tweepy.Cursor(api.user_timeline,\n","#                                 screen_name=market,\n","#                                 tweet_mode='extended',\n","#                                 count=1000).items(1000)\n","#         for tweet in tweets:\n","#             market_tweets_dict[market] = market_tweets_dict.get(market, []) + [tweet._json]\n","#     except:\n","#         pass\n","\n","# # Write file to disk\n","# with open('./data/market_tweets_dict.data', 'wb') as filehandle:\n","#     pickle.dump(market_tweets_dict, filehandle, protocol=pickle.HIGHEST_PROTOCOL)\n","\n","# print('Number of markets:', len(list(market_tweets_dict.keys())))\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":["# Selection and labeling of farmers market tweets\n"," Here, we wish to first select only those markets that have tweeted over 500\n"," times. If it isn't, we will drop that market. If it is, we will transfer it\n"," over into a dictionary of the form\n"," ```\n"," {\n","     screen_name: {\n","                     'tweets': [{tweet1_json}, ..., {tweet}],\n","                     'label': 1\n","                  }\n"," }\n"," ```"],"metadata":{}},{"source":["with open('./data/market_tweets_dict.data', 'rb') as filehandle:\n","    market_tweets_dict = pickle.load(filehandle)\n","\n","dataset_tweets_random_users = {}\n","for market in market_tweets_dict:\n","    dataset_tweets_random_users[market] = {}\n","    \n","    if len(market_tweets_dict[market]) >= 500: # If the market has >=500 tweets\n","        dataset_tweets_random_users[market]['tweets'] = market_tweets_dict[market]\n","        dataset_tweets_random_users[market]['label'] = 1"],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[" We still have about 485 valid markets with over 500 tweets, so this is still\n"," something to work with. Next we find random accounts from\n"," different geographic locations across the country to add the datasets\n"," labeled 0."],"metadata":{}},{"cell_type":"markdown","source":[" ### Finding random users to label as 0.\n"," We generate a user_id at random and download up to 500 tweets. We then check\n"," if there are infact 500 tweets, and if the user has at least 300 followers.\n"," If not, we pass on that user. If yes, we add the user to `dataset_tweets`\n"," using the appropriate dictionary structure. We do this until we have 500\n"," random users."],"metadata":{}},{"source":["# num_selected = 0\n","# while num_selected < 500:\n","#     random_id = id_generator()\n","#     try:\n","#         user = api.get_user(user_id=random_id,\n","#                                     lang='en',\n","#                                     include_entities=True)\n","#         # tweets = tweepy.Cursor(api.user_timeline,\n","#         #                         user_id=random_id,\n","#         #                         tweet_mode='extended',\n","#         #                         count=500).items(500)\n","#         user = user._json\n","#         if user['followers_count'] >= 300 and user['statuses_count'] >= 500:\n","#             num_selected += 1\n","#             print(num_selected)\n","#             screen_name = user['screen_name']\n","#             dataset_tweets_random_users[screen_name] = {}\n","#             dataset_tweets_random_users[screen_name]['label'] = 0\n","#             tweets = tweepy.Cursor(api.user_timeline,\n","#                                     user_id=random_id,\n","#                                     tweet_mode='extended',\n","#                                     count=500).items(500)\n","#             for tweet in tweets:\n","#                 dataset_tweets_random_users[screen_name]['tweets'] = (\n","#                     dataset_tweets_random_users[screen_name].get('tweets', []) \n","#                                                                 + [tweet._json])\n","#     except:\n","#         pass\n","\n","# with open('./data/dataset_tweets_random_users.data', 'wb') as filehandle:\n","#     pickle.dump(dataset_tweets_random_users, filehandle, \n","#                 protocol=pickle.HIGHEST_PROTOCOL)\n","\n","with open('./data/dataset_tweets_random_users.data', 'rb') as filehandle:\n","    dataset_tweets_random_users = pickle.load(filehandle)\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":["  ## Building more suitable dictionary format to ingest later.\n"," Note that the structure of the `dataset_tweets_random_users` dictionary is\n"," ```\n"," {\n","     screen_name: {\n","                     'tweets': [{tweet1_json}, ..., {tweet}],\n","                     'label': 1\n","                  }\n"," }\n"," ```\n"," We now want to build all the usual data cleaning tokenziation, lemmatization\n"," pipeline and then build the corpus of words. For this, I am just copying and\n"," functions from `LDA.ipynb`. Ideally, I would want to have a `utils` file and\n"," then just import functions as needed from `utils.py`. With more time, I can\n"," probably clean up my code base significantly. Dividing the code into those\n"," that download data and then export things a dictionaries, and those that\n"," utilize functions from `utils.py` to keep everything clean and modular.\n"," I finally want a dictionary of the form\n"," ```\n"," {\n","     user: {\n","               'hashtags': [..., ..., ...],\n","               'full_text': [cleaned, tokenized, lemmatized, words of tweets],\n","               'label': 0 or 1\n","           }\n"," }\n"," ```\n"," We have defined some utility functions above to make this easier - this has\n"," been copied over from `LDA.py`. Ideally I want these in a utils file so that\n"," I can just import from there rather than repeating functions."],"metadata":{}},{"source":["lda_dict_random_users = {}\n","counter = 0\n","for user in tqdm(dataset_tweets_random_users):\n","    try:\n","        lda_dict_random_users[user] = {}\n","        lda_dict_random_users[user]['hashtags'] = []\n","        lda_dict_random_users[user]['fulltext'] = []\n","\n","        \n","        tweets = dataset_tweets_random_users[user]['tweets']\n","        label = dataset_tweets_random_users[user]['label']\n","\n","        for tweet in tweets:\n","            hashtags = get_hashtag_list(tweet)\n","            words = get_tweet_words_list(tweet)\n","\n","            lda_dict_random_users[user]['hashtags'].extend(hashtags)\n","            lda_dict_random_users[user]['fulltext'].extend(words)\n","            lda_dict_random_users[user]['label'] = label\n","    except:# Just skip the user if something goes wrong\n","        pass\n","\n","# if you want to write the data to disk.\n","\n","with open('./data/lda_dict_random_users.data', 'wb') as filehandle:\n","    pickle.dump(lda_dict_random_users, filehandle, \n","                protocol=pickle.HIGHEST_PROTOCOL)\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3}},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3}}