{"cells":[{"cell_type":"markdown","metadata":{},"source":[" # Identifying commercial markets to filter them out from followers\n","\n"," The goal of this file is to be able to identify which of the followers that I\n","  selected are commercial followers or otherwise small businesses. This\n"," corrupts my input user base with non-people, so this is an attempt to remove\n"," these followers."]},{"cell_type":"markdown","metadata":{},"source":[" ## Imports"]},{"cell_type":"code","execution_count":0,"metadata":{},"outputs":[],"source":["import json\n","import glob\n","import pickle\n","import collections\n","import random\n","from tqdm import tqdm as tqdm\n","import time\n","\n","import os\n","dirpath = os.path.dirname(os.path.realpath('__file__'))\n","\n","import tweepy\n","import config\n","\n","import nltk\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","\n","import shapefile\n","from shapely.geometry import Point # Point class\n","from shapely.geometry import shape # shape() is a function to convert geo objects through the interface\n",""]},{"cell_type":"markdown","metadata":{},"source":["# All functions"]},{"cell_type":"code","execution_count":0,"metadata":{},"outputs":[],"source":["def is_urban(pt, shapes):\n","    \"\"\"\n","    Takes in a point and then checks to see if it lies within an urban boundary\n","    returns: Boolean\n","    \"\"\"\n","    for boundary in shapes:\n","        if Point(pt).within(shape(boundary)): \n","            return True\n","    return False\n","\n","def handle_applier(handle):\n","    \"\"\"\n","    Some twitter handles have the '@' symbol which shouldn't be \n","    passed to the twitter API so we will remove them\n","    \"\"\"\n","    if len(handle) == 0:\n","        return handle\n","    if handle[0] == '@':\n","        return handle[1:]\n","    return handle\n","\n","def id_generator():\n","    \"\"\"\n","    Generates user_id integers to supply to twitter while generating random\n","    users. Note that the docs say that these numbers are 64 bit unsigned\n","    integers (https://developer.twitter.com/en/docs/basics/twitter-ids)\n","    input: none\n","    output: a random 64-bit unsigned integer\n","    \"\"\"\n","    # Pick bit length at random\n","    bit_length = np.random.randint(low=10, high=50)\n","    \n","    # Pick the bits at random\n","    digits = np.random.randint(low=0, high=2, size=bit_length)\n","    binary_number = ''.join(list(map(str, digits)))\n","    return int(binary_number, 2)\n",""]},{"cell_type":"markdown","metadata":{},"source":[" ## Loading the data and adding an `is_urban` column\n","\n"," We get the data of urban regions from US Census data, and we write a simple\n"," for loop to see if a particular city lies in an urban region. I only want to\n"," select users from urban areas."]},{"cell_type":"code","execution_count":0,"metadata":{},"outputs":[],"source":["load_columns = ['Twitter', 'city', 'State', 'x', 'y', 'zip' ]\n","markets = pd.read_csv('./farmers_market_twitter_all.csv', usecols=load_columns)\n","markets = markets.dropna(axis=0)\n","markets['coords'] = markets.apply(lambda row: (row['x'], row['y']), axis=1)\n","\n",""]},{"cell_type":"code","execution_count":0,"metadata":{},"outputs":[],"source":["# shp = shapefile.Reader('./data/tl_2019_us_uac10/tl_2019_us_uac10.shp')\n","# shapes = shp.shapes() # get all the polygons\n","\n",""]},{"cell_type":"code","execution_count":0,"metadata":{},"outputs":[],"source":["# tqdm.pandas()\n","# markets['is_urban'] = markets.progress_apply(lambda row: is_urban(row['coords'],\n","#                                                             shapes), axis=1)\n","# markets = markets[markets['is_urban']] # select only the urban_areas\n","# print('Number of eligible markets:', markets.shape[0])\n","# # export to csv so I don't have to do this again\n","# markets.to_csv('./data/markets_is_urban.csv', index=False)\n","\n","markets = pd.read_csv('./data/markets_is_urban.csv')\n",""]},{"cell_type":"markdown","metadata":{},"source":[" ### I still need to further process the twitter handle because they are all over the place"]},{"cell_type":"code","execution_count":0,"metadata":{},"outputs":[],"source":["\n","# Remove all the '#NAME?' entries. Decreases the number of markets to 789.\n","markets = markets[markets['Twitter'] != '#NAME?']\n","# Only extract the bits after the last slash\n","markets['Twitter'] = markets['Twitter'].apply(lambda r: r.rsplit('/', 1)[-1])\n","markets['Twitter'] = markets.apply(lambda r: handle_applier(r['Twitter']), \n","                                                            axis=1)\n",""]},{"cell_type":"markdown","metadata":{},"source":[" ## Download upto a 1000 tweets for farmers markets that lie in urban areas."]},{"cell_type":"code","execution_count":0,"metadata":{},"outputs":[],"source":["consumer_key = config.consumer_key\n","consumer_secret = config.consumer_secret\n","auth = tweepy.AppAuthHandler(consumer_key, consumer_secret)\n","\n","api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)\n","\n",""]},{"cell_type":"code","execution_count":0,"metadata":{},"outputs":[],"source":["# In future runs, if you don't have to download this data again,\n","# just load the original pickle file\n","# market_tweets_dict = {}\n","# for market in tqdm(markets['Twitter']):\n","#     try:\n","#         tweets = tweepy.Cursor(api.user_timeline,\n","#                                 screen_name=market,\n","#                                 tweet_mode='extended',\n","#                                 count=1000).items(1000)\n","#         for tweet in tweets:\n","#             market_tweets_dict[market] = market_tweets_dict.get(market, []) + [tweet._json]\n","#     except:\n","#         pass\n","\n","# # Write file to disk\n","# with open('./data/market_tweets_dict.data', 'wb') as filehandle:\n","#     pickle.dump(market_tweets_dict, filehandle, protocol=pickle.HIGHEST_PROTOCOL)\n","\n","# print('Number of markets:', len(list(market_tweets_dict.keys())))\n",""]},{"cell_type":"markdown","metadata":{},"source":["# Selection and labeling of farmers market tweets\n"," Here, we wish to first select only those markets that have tweeted over 500\n"," times. If it isn't, we will drop that market. If it is, we will transfer it\n"," over into a dictionary of the form\n"," ```\n"," {\n","     screen_name: {\n","                     'tweets': [{tweet1_json}, ..., {tweet}],\n","                     'label': 1\n","                  }\n"," }\n"," ```"]},{"cell_type":"code","execution_count":0,"metadata":{},"outputs":[],"source":["with open('./data/market_tweets_dict.data', 'rb') as filehandle:\n","    market_tweets_dict = pickle.load(filehandle)\n","\n","dataset_tweets_random_users = {}\n","for market in tqdm(market_tweets_dict):\n","    dataset_tweets_random_users[market] = {}\n","    \n","    if len(market_tweets_dict[market]) >= 500: # If the market has >=500 tweets\n","        dataset_tweets_random_users[market]['tweets'] = market_tweets_dict[market]\n","        dataset_tweets_random_users[market]['label'] = 1"]},{"cell_type":"markdown","metadata":{},"source":[" We still have about 485 valid markets with over 500 tweets, so this is still\n"," something to work with. Next we find random accounts from\n"," different geographic locations across the country to add the datasets\n"," labeled 0."]},{"cell_type":"markdown","metadata":{},"source":[" ### Finding random users to label as 0.\n"," We generate a user_id at random and download up to 500 tweets. We then check\n"," if there are infact 500 tweets, and if the user has at least 300 followers.\n"," If not, we pass on that user. If yes, we add the user to `dataset_tweets`\n"," using the appropriate dictionary structure. We do this until we have 500\n"," random users."]},{"cell_type":"code","execution_count":0,"metadata":{},"outputs":[],"source":["num_selected = 0\n","t1 = time.time()\n","while num_selected < 500:\n","    random_id = id_generator()\n","    try:\n","        user = api.get_user(user_id=random_id,\n","                                    lang='en',\n","                                    include_entities=True)\n","        # tweets = tweepy.Cursor(api.user_timeline,\n","        #                         user_id=random_id,\n","        #                         tweet_mode='extended',\n","        #                         count=500).items(500)\n","        user = user._json\n","        if user['followers_count'] >= 300 and user['statuses_count'] >= 500:\n","            num_selected += 1\n","            print(num_selected)\n","            screen_name = user['screen_name']\n","            dataset_tweets_random_users[screen_name] = {}\n","            dataset_tweets_random_users[screen_name]['label'] = 0\n","            tweets = tweepy.Cursor(api.user_timeline,\n","                                    user_id=random_id,\n","                                    tweet_mode='extended',\n","                                    count=500).items(500)\n","            for tweet in tweets:\n","                dataset_tweets_random_users[screen_name]['tweets'] = (\n","                    dataset_tweets_random_users[screen_name].get('tweets', []) \n","                                                                + [tweet._json])\n","    except:\n","        pass\n","t2 = time.time()\n","print('Time elapsed for download:', t2-t1)\n","\n","with open('./data/dataset_tweets_random_users.data', 'wb') as filehandle:\n","    pickle.dump(dataset_tweets_random_users, filehandle, \n","                protocol=pickle.HIGHEST_PROTOCOL)"]}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3}},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3}}